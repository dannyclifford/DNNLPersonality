{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone Project\n",
    "Danny Clifford\n",
    "October 2018\n",
    "\n",
    "\n",
    "# I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "With much better memory than people and the amount of personal information we share with computers, it's amazing they don't appear to understand our personalities better.  With the exceptions of saving searches and some companies using AI and machine learning to predict profitability for advertisements, there is little effort to understand personal tendencies to cater content for (and not just to) individuals. Even those multibillion dollar corporations aim to maximize advertisment profit, not to understand the fundamental features that make each of us unique.  \n",
    "\n",
    "Understanding what connects us all, what makes each of us unique, what are our strengths and how can we harness everyone's strengths to build better lives for individuals and humanity, seem's far from the focus of their efforts.  For the first time in history, we are able to economically collect and process enough information to understand the patterns of human nature.  There have been good attempts in recent history of distinguishing the features of human processing that make certain people different than others, most notably Carl Jung in his book Psychological Types and the subsequent adaptation of his theories to the Myers Briggs Personality Index.  \n",
    "\n",
    "Carl Jung notes that it is difficult for a person who experiences their own bias to accurately judge others, joking that one person creating a system would be like creating a Universal Church with one member.  Luckily, since his time wonderful scientists such as Alan Turing, John von Neumann, J.C.R. Licklider, Miller, Moore, Noyce, and countless other have made it incredibly easy to collect, share, and calculate data from around the world almost instantly, not to mention make impressive improvements on models of understanding how agents behave.  With the addition of breakthroughs in behavioral psychology by greats like Kahnemann, Tversky, and Thaler, we are quickly building the ability to study the patterns of reason and thought in humans as differentiated by the rational agents which traditional economic theory implies.\n",
    "\n",
    "Putting these pieces together, as we communicate with computers and people, we are creating valuable information and patterns that, if only captured and studied, would give great insight into how our individual and collective minds work.  This project is about helping individuals use computers understand how the patterns in our language reflect our inner personality and in turn how we recieve, process, and communicate information; which determines the outcomes in our personal lives and compounded over every individual over time, ammounts to the fate of humankind.  Ultimately, computers can be our tools to help us learn our unique patterns and to help us change, supplement, or leverage how we do things to help us solve problems and achieve our goals.\n",
    "\n",
    "### Problem Statement\n",
    "    \n",
    "The problem that I am setting out to solve is how to understand someone's personality based on their use of language.  If we can accurately predict one of the most fundamental aspects of a personâ€™s behavior and uniqueness as measured by the language they use, the ability to communicate information with that person will be drastically improved.  The internet is designed based on information that is already programmed into the web page itself; for example, administrators see a website much differently as a new customer or even a logged in user and are determined prior to visiting the webpage. This poses a difficult problem for web designers to incorporate designs that maximize the profit or usefulness to their intended audience rather than communicating information or value to a person on an individual basis.  If we can predict learning style or how an individual will react to their environment, then we can better customize the learning experience to their preferences and strengths. \n",
    "    \n",
    "Quantifying personality has been done for us with the Myers Brigg Personality Index 4 letter code. These will be further broken down into their 4 features of a single letter with only 2 options.  In addition, it will also allow us to train the weights of determining individual features of personality in a more focused way. NLP allows algorithms to extract meaning from text whether from word count, frequency, parts of speech, and even sentiment in a quantifiable and measurable way. These matrices of language data will be learned by a Deep Neural Network and these patterns recognized during training will be used to predict the personality features of the test group.  \n",
    "\n",
    "1. Download the data from Kaggle\n",
    "2. Let SpaCy web-medium language model run through the posts to make word vectors\n",
    "3. Shuffle-Split the data into testing, validation, and training data\n",
    "4. Run benchmarks training and testing with Logistic Regression, Random Forest, and MLP Classifiers\n",
    "5. Run training and validation through the DNN\n",
    "6. Test accuracy of the models on the test set with AUC\n",
    "  \n",
    "    \n",
    "### Metrics\n",
    "\n",
    "Training a neural network on language use and their corresponding personality feature labels allows us to measure the AUC.  Area under the ROC curve is used to ensure the proper binary classification when it comes to specificity and sensitivity. This will help better quantify individual differences in each of the 4 personality features. Wang uses AUC in order to quantify and measure accuracy of his models.  Since the distribution of personalities within the dataset is skewed in both our datasets, this will be a good evaluation metric to use.  He also measured accuracy by comparing different models based on the features mentioned above based on AUC, not only breaking them down into dichotomous features (Sensing and Intuitive, Extrovert and Introvert), but also by focusing on features of the language.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "The dataset is taken from Kaggle and contains 8,600 users with 50 recent comments on the Kaggle website each and their corresponding personality type.  This was user generated data from the Kaggle website and offers the most labeled personality data connected to their text data (comments) of what I could find online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the excel document with the results\n",
    "# save it as a panda's dataframe and call it data\n",
    "data = pd.read_csv('raw/mbti_1.csv')\n",
    "\n",
    "# print out a summary of the first 5 people to make sure it worked\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first 5 entries of the Kaggle dataset. It comes with a 4 letter code called 'type' which is the person's peronality archetype according to their test results from the Myers Briggs Personality Index. Under 'posts' is a string of their most recent 50 posts on Kaggle.com separated by |||. \n",
    "\n",
    "Below is a look at the first person's entire corpus of text that we can learn from in raw form. In Data Preprocessing we will remove the links and ||| along with creating a bag of words that a the person uses that we can compare to other people and personality types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The bar graph below shows the distribution of the personality types in the Kaggle database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = data['type'].value_counts()\n",
    "k = data['type'].value_counts().keys()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(d.index, d.values, alpha=0.7)\n",
    "plt.ylabel('Instances', fontsize=12)\n",
    "plt.xlabel('Types', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly Introverted and/ or iNtuitive people dominate the Kaggle forums, or at least the ones participating in the creation of the database.  This will make it quite difficult to learn about those who are extroverted and sensing types (ESxx).  The model would likely minimize error by simply never predicting ESxx labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution = {}\n",
    "#actual_series = {}\n",
    "\n",
    "l = float(len(data))\n",
    "actual = [0.044, 0.015, 0.033, 0.021, 0.032, 0.081, 0.054, 0.088, 0.018, 0.116, 0.025, 0.138, 0.043, 0.085, 0.123, 0.087]\n",
    "kaggle = []\n",
    "\n",
    "for i in range(len(d)):\n",
    "    kaggle.append(float(d[i]) / l)\n",
    "\n",
    "keys = d.index.get_values()\n",
    "values = d.get_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEMCAYAAAClRuMkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXn8llP+/5+v9iIpxaS0IEvZlTAma8RQtsgagyyT5WuGYRhiMHZjG0TZ9yhFg8HYZkJFP8QYSfhgSBHJ0vL+/XHOXVd393Ldnz73Z30/H4/7cV/XOed6X+/rus513ue8z7nOkZnhOI7jOHWNRjWtgOM4juNUBjdgjuM4Tp3EDZjjOI5TJ3ED5jiO49RJ3IA5juM4dRI3YI7jOE6dxA1YASQdJunpmtajNiLpDkkXxe1fSXqvms8/XdJOVSRruecsySStXxWyo7z5ktatKnkpz9lS0gRJ8yQ9XJ3nLkZV318nIOkoSS/XtB5pKKarpOclHVtMTrUYMEmHSpoSX+TPJf1d0g7Vce6VwczuNbPdyyFb0ixJP8R78oWk2yWtWo5zlRsze8nMNszsx2vbrTKyJHWLBdz8xL15XFL/rHP2MrPnU8pqUkT/KnvOuV48M1vVzGZWhfwSOBBYC1jDzAZnR0oaIemexH4nSf+RdJ0kVaeiaYmF3uJE3vgwvjcblCBjacWrnKQ5z8oY8rR5u5KyO0bZayXCzskT9mRVn78Uym7AJJ0O/BW4hPBCdQH+Bgwq97lXhnJkjBzsY2arAlsBfYBzSxVQTXrWBKvHe7M58A9grKSjqvok9fj+dQX+a2aLiiWU1BV4ERhvZqdY7Z7dYFLMF22A3YAfgKmSNqlZteoPZvY5MAPolwjuB/wnR9iLpcqv0nfOzMr2I2Sy+cDgAmmaEwzcZ/H3V6B5jNsJqADOBL4EPgf2BfYC/gvMBf6YkDUCGAM8CHwHvA5snog/C/ggxr0D7JeIOwr4F3BNlHtRDHs5kcaAE4D3ga+BGwHFuMbAVcBXwIfA8Ji+SZ7rngXslti/Ang8cd9Gxev9NOrSuICe6wMvAPPi+R9MyN0emBzjJgPbJ+KeB/4c5X0HPA20T8Q/DPwvHvsi0CsRdwdwUfI5xe27gSWEgmV+fHZPACdnXf+bwL457ku3XPcN+D3wBdAo+/4B2wBTgG9jmqtj+MdR1vz4266E53wKMDPezysS5x0B3JNLX+BiYDHwYzzfDQl56yee7V3AbOAjQqWlUeLZvgxcSchfHwJ7Fnh3No7P8BtgOjAwhl8A/AwsjHock+PYEcA9wHpRjz9nxR8NvBvzxUzg+Kz4Mwn58zPg2KxrXAOYEJ/H5HiPs+9vJm3zeL0fx2d3M9Ayz/Uu95wS4Y8DY4rlW2BYvCc/x/syIUW5UOjd2ohQuZoLvAccVOg8WTq/GO/D9zHNwTH8OILxmAuMB9bOcy/y5e28+YcC5UoO+aOA6xNl25eEsi8Z9i2wQ8p8Xeyd608wkPOAG+I9P7aojSmWYGV+wABgEXkK8ZjmQuAVYE2gA/Bv4stEKBgXAecBTePDnQ3cB7QGehEKi3UTL+VCgvukKaHQ+xBoGuMHA2sTWp4Hx8zTMXGTFwEnEwqjljlushFeltUJLcnZwIAYdwIh83cG2gLPkNKAAesQCqDMdY8DbgFWifflNWIBkkfP+4Fz4nW1SGSqdoSMfERMe0jcXyPGP094cTeIcp4HLk3o+Jt4nzOVjGmJuDvIYcCyry3uHwS8mtjfHJgDNMtxX7rlum/AujF84xz3bxJwRNxeFdg2n6wSnvM/4/3rQqgsHZvIYzkNWOKeHpule7LAvgt4LN7XblH2MQndFhLyeWPgRIKBUI771JRQ0P0RaAbsQiiAN8ylZ47jRxDetU9JVAIT8b8mGDcBOwILgK0S7/X/CO9fK0KlJXmND8RfK6An8EmO+5tJ+1dCQd0u3pMJwF/y6Lzcc8rKp1+Umm8TYYXKhXzv1irxuo4m5KOtCAauV77z5NB76X2I+7tEGVtF3a8HXsxzbDdy5+28+YcC5UoO+UOB/xe3exMMbo+ssB+I7zDF83Xedw5oTzCGmXL7/2L6GjdghwH/K5LmA2CvxP4ewKy4vVO8SZnWR+v40Pom0k8l1uQJL+UribhGhNrGr/KcexowKHGTPy70wsRz75DYfwg4K24/l8wMBPdGMQM2n1B7/ojgVm1JcLP+RKIWSjA8/yyg513ASKBzVvgRwGtZYZOAo+L288C5ibiTgCfz6Lt6vJ422S8oxQ1Yc0LNq0fcvxL4W9oXM4a3iOG/zD4H4eW6gETrschLnuY5D8i6L88m8lilDBihUPkJ6JmIOx54PqHHjERcq3jsL3Lcp18RjEijRNj9wIhceuY4fgSh0PgGWC/FuzwOODVujyZhZOK1Ja9xIdGQxvicLTCCcfw+eX5CS+LDPDos95wS4QOAhaXm2wLXmiwX8r1bBwMvZYXdApxfwnmyDdgo4PLE/qrxXnZL854Uyj8UKVfyyF9MqIz/H3BxDP80EfbPGJYmX+d954AjWb7cFsHzVtSAlbsPbA7QvojPc21CAZ7hoxi2VIaZLY7bP8T/LxLxPxAedIZPMhtmtoRwI9YGkHSkpGmSvpH0DbAJwfqvcGwB/pfYXpA499pZx6eRta+ZrW5mXc3sJDP7gdB30RT4PKHnLYQaUz7ZZxIe+mtxdN5vEjp9lJX2I6BTseuR1FjSpZI+kPQtwWDA8vcrFWb2E8HYHy6pEeHFubtEMRmd5+aIO4bQivyPpMmS9i4iK82zSabJzpOVpT2htZSd33M+DzNbEDdzDe5ZG/gk5vF8sooxnmCMnov9YEuRtKekVyTNjXlwL5Y9+0J5vQOhlp3mXehAKGSnJvL6kzG8FDoR80Vl8m2RciHfu9UV6Js5Jh53GMFYVJbl3lczm08oQ0t5pvnyT5pyhcSxswhl5w6Evq6XYtSkRFim/ytNvi70zi2XnyxYsTTvKOXuwJ5EcPHtS+ibysVnhJs7Pe53iWGVZZ3MRiwsOwOfxRf0VmBXQkfwYknTCJkzg63EeT+P51pBjxL5hFCbaW/5O+CX09PM/kdwGxBHdz4j6UWW3dskXQiFRDEOJQy02Y1QCLQhuB/TjFDLdR/vJBitl4EFZjYphZwk+xH88CsM1zez94FD4vPeHxgjaY08euTTL5uMWxeWz5PfEwrdDNkFViHZXxFq1F0J7uaM7E9T6JPNZ8A6kholjFjG3ZkaMztdUnOCEetnZp/G/UcINePHzGyhpHEse/aF8vpsgvunc0KXfO/CV4QKaC8zq8w9yLAfywrYYvl2uedTrFwo8G59ArxgZsuNjk1QmbJkufdV0iqE/sRc96ZU+WnKlWxeIhiq7QguxWTYDoS+KkiXrwvp+znLl9siZflZ1haYmc0j9F/dKGlfSa0kNY21u8tjsvuBcyV1kNQ+pr8nn8wUbC1p/9jqO43w0F4h+H2N8IIh6WhCTauqeAg4NQ5HXh34Q2WEWBgB9DRwlaTVJDWStJ6kHfMdI2mwpEyB8jXhOhcDE4ENFD5jaCLpYEKfxOMpVGlNuHdzCAX2JSVcxheEPqvkdU0iDO64ihJaX5LWkjQcOB84O6vFkUlzuKQOMe6bGLyY8KyXZOuSkjMktZW0DnAqYWAQBPdSP0ldJLUBzs46boVrzxA9CQ8BF0tqHQvP06lcfn+VYEzPjO/UTsA+hL6nUhlOcIE/G4dJNyO4fWcDiyTtCSQ/M3gIOFrSxpJaEd5ZYOk1PgqMiO/7RgRDuALxed0KXCNpTVg6nH+PYgrHllZ3SdcTXNgXxKhi+Tb7+RQsFwq8W48T3q0j4v1vKqmPpI3znCcX2WnuI9zXLWIl4hJC3/GsHMeWlLcrU64QWlhHAp+Z2bcx7OUY1obQQKmKfP0E0CtRbp9CypZs2YfRm9nVhIs5l3DTPyG8MONikosII8jeBN4ijBxcme80HiP4pzODF/Y3s4Vm9g6h8JxEyDibEkbGVBW3EjLIm8AbBOOxiJDZS+VIQiHyDuE6xgAdC6TvA7wqaT7BLXSqmX1oZnOAvYHfEV7oM4G9zeyrFDrcRXADfBr1eKUE/f9CqJR8I+n3WTI3JV3G/kbS94Q8sRdhJOvoPGkHANPj9V8LDDGzH6ML5WLgX1GXbUu4hscI/avTCC/YKAAz+wfBmL0Z47MrA9cCB0r6WtJ1OeSeTDA8MwmFwX0EN15JmNnPwEBgT0IN+G/AkWb2n0rIMkKfxWuEwUfNCYXIQ4T8dyghX2XS/x24jjDQZQaxICMYDgjvdxuCO+tuQiU1E5fNH6KMV6LL7xlgwzxpAbaLz/lbQn/jakAfM3srxhfLt6OAnjE/jEtRLuR7t74jGPUhhJbT/4DLCPduhfPkuZYRwJ0xzUFm9izwJ0Lr93PCIJohuQ6sZN4utVx5geBiTH5wPI3QVz814aKElcjXsTwaDFxKKKd6kLJszoxOqRdIGkHoFD28FuiyJ3CzmWW78Bosko4EhplZrf+I3UlPbHW8Tfj8ZQX3lKTLCANRhq5wsOOsBD6VVBWhMHXPXtFV14ng8hpb03rVFqKr6STCiC6njiNpP0nNJLUltDwmZIyXpI0kbabANoRBNv4uOFWOG7CqQwQ//NcEF+K7JPoGGjKxT2M2wUVzXw2r41QNxxOe6QcEN/mJibjWhH6w7wluyKsILlnHqVLqlQvRcRzHaTh4C8xxHMepk9TXiUyXo3379tatW7eaVsNxHKdOMXXq1K/MrNQPy6uNBmHAunXrxpQpU2paDcdxnDqFpOyZfGoV7kJ0HMdx6iRuwBzHcZw6iRswx3Ecp07SIPrAHMdxABYuXEhFRQU//vhjTatSq2jRogWdO3emadOmNa1KSbgBcxynwVBRUUHr1q3p1q0bYdJzx8yYM2cOFRUVdO/evabVKQl3ITqO02D48ccfWWONNdx4JZDEGmusUSdbpW7AHMdpULjxWpG6ek/cgDmO4zh1Eu8DK8I+++SPmzCh+vRwHKfqKfR+V4Y0ZcKqq67K/PnzAZg4cSKnnnoqzz77LF26dKkyPY466ij23ntvDjzwwCqTWRtxA+Y4jlMDPPvss5x88sk8/fTTVWq8GhLuQnQcx6lmXnrpJY477jieeOIJ1ltvPQAmTJhA37592XLLLdltt9344osvAJg9ezb9+/dnq6224vjjj6dr16589VVYVP3Pf/4zG220Ef379+eQQw7hyiuvXOFcU6dOZccdd2Trrbdmjz324PPPP6++Cy0zbsAcx3GqkZ9++olBgwYxbtw4Ntpoo6XhO+ywA6+88gpvvPEGQ4YM4fLLLwfgggsuYJddduH1119nv/324+OPPwZgypQpPPLII7zxxhs8+uijOed7XbhwISeffDJjxoxh6tSp/OY3v+Gcc86pngutBtyF6DiOU400bdqU7bffnlGjRnHttdcuDa+oqODggw/m888/5+eff176TdbLL7/M2LFhQesBAwbQtm3bpeGDBg2iZcuWAOyTo0Pvvffe4+2336Z///4ALF68mI4dO5b1+qoTb4E5juNUI40aNeKhhx5i8uTJXHLJJUvDTz75ZIYPH85bb73FLbfcsvS7rHyLDqdZjNjM6NWrF9OmTWPatGm89dZbPP3001VzIbWAshowSQMkvSdphqSzcsSfLukdSW9KelZS10TcUEnvx9/QRPjWkt6KMq9TXf2AwXGcBkurVq14/PHHuffeexk1ahQA8+bNo1OnTgDceeedS9PusMMOPPTQQwA8/fTTfP3110vDJ0yYwI8//sj8+fN54oknVjjPhhtuyOzZs5k0aRIQXIrTp08v67VVJ2VzIUpqDNwI9AcqgMmSxpvZO4lkbwC9zWyBpBOBy4GDJbUDzgd6AwZMjcd+DdwEDANeASYCA4C/l+s6HMepv9TkpzDt2rXjySefpF+/frRv354RI0YwePBgOnXqxLbbbsuHH34IwPnnn88hhxzCgw8+yI477kjHjh1p3bo1ffr0YeDAgWy++eZ07dqV3r1706ZNm+XO0axZM8aMGcMpp5zCvHnzWLRoEaeddhq9evWqiUuucpSmGVopwdJ2wAgz2yPunw1gZn/Jk35L4AYz+6WkQ4CdzOz4GHcL8Hz8/dPMNorhy6XLR+/eva2yC1r6d2COU39499132XjjjWtajZL46aefaNy4MU2aNGHSpEmceOKJTJs2DYD58+ez6qqrsmDBAvr168fIkSPZaqutKnWeXPdG0lQz673SF1EmyjmIoxPwSWK/AuhbIP0xLGtJ5Tq2U/xV5Ah3HMepl3z88cccdNBBLFmyhGbNmnHrrbcujRs2bBjvvPMOP/74I0OHDq208aqrlNOA5eqbytnck3Q4wV24Y5FjS5E5jOBq9I8EHceps/To0YM33ngjZ9x9991XzdrULso5iKMCWCex3xn4LDuRpN2Ac4CBZvZTkWMr4nZBmQBmNtLMeptZ7w4dOlT6IhzHcZzaSTkN2GSgh6TukpoBQ4DxyQSx3+sWgvH6MhH1FLC7pLaS2gK7A0+Z2efAd5K2jaMPjwQeK+M1OI7jOLWUsrkQzWyRpOEEY9QYGG1m0yVdCEwxs/HAFcCqwMNxNPzHZjbQzOZK+jPBCAJcaGZz4/aJwB1AS0KfmY9AdBzHaYCUdSYOM5tIGOqeDDsvsb1bgWNHA6NzhE8BNqlCNR3HcZw6iE8l5ThOw6Um1lMBxo4dy/7778+777673HyI2dxxxx3svvvurL322pVS5/nnn+fKK6/k8ccfr9TxtR2fSspxHKeauf/++9lhhx144IEHCqa74447+OyznOPUHNyAOY7jVCvz58/nX//6F6NGjVrOgF1++eVsuummbL755px11lmMGTOGKVOmcNhhh7HFFlvwww8/0K1bt6VLqUyZMoWddtoJgNdee43tt9+eLbfcku2335733nuvJi6t2nEXouM4TjUybtw4BgwYwAYbbEC7du14/fXX+eKLLxg3bhyvvvoqrVq1Yu7cubRr144bbriBK6+8kt69C0+GsdFGG/Hiiy/SpEkTnnnmGf74xz/yyCOPVNMV1RxuwBzHcaqR+++/n9NOOw2AIUOGcP/997NkyRKOPvpoWrVqBYR5Ekth3rx5DB06lPfffx9JLFy4sMr1ro24AXMcx6km5syZw3PPPcfbb7+NJBYvXowkDjjgANIsrNGkSROWLFkCsHS5FYA//elP7LzzzowdO5ZZs2YtdS3Wd7wPzHEcp5oYM2YMRx55JB999BGzZs3ik08+oXv37rRr147Ro0ezYMECAObODZ+9tm7dmu+++27p8d26dWPq1KkAy7kIk0ux3HHHHdV0NTWPt8Acx2m4VPOSEvfffz9nnbX80ogHHHAA7777LgMHDqR37940a9aMvfbai0suuYSjjjqKE044gZYtWzJp0iTOP/98jjnmGC655BL69l02N/qZZ57J0KFDufrqq9lll12q9ZpqkrItp1Kb8OVUHMeBurmcSnVRF5dTcRei4ziOUydxA+Y4juPUSdyAOY7ToGgI3SalUlfviRswx3EaDC1atGDOnDl1tsAuB2bGnDlzaNGiRU2rUjJFRyFKag4cAHRLpjezC1McOwC4lrCcym1mdmlWfD/gr8BmwBAzGxPDdwauSSTdKMaPk3QHYeXmeTHuKDObVkwXx3Gczp07U1FRwezZs2talVpFixYt6Ny5c/GEtYw0w+gfIxiLqcBPRdIuRVJj4EagP2El5cmSxpvZO4lkHwNHAb9PHmtm/wS2iHLaATOApxNJzsgYO8dxnLQ0bdqU7t2717QaThWRxoB1NrMBlZC9DTDDzGYCSHoAGAQsNWBmNivGLSkg50Dg72a2oBI6OI7jOPWUNH1g/5a0aSVkdwI+SexXxLBSGQLcnxV2saQ3JV0TXZyO4zhOAyONAdsBmCrpvWg03pL0Zorjck3sVVLPqaSOwKbAU4ngswl9Yn2AdsAf8hw7TNIUSVPc3+04jlP/SONC3LOSsiuAdRL7nYFSV2Y7CBhrZkunVjazz+PmT5JuJ6v/LJFuJDASwkwcJZ7XcRzHqeUUbYGZ2UfA6sA+8bd6DCvGZKCHpO6SmhFcgeNL1O8QstyHsVWGwtTN+wJvlyjTcRzHqQcUNWCSTgXuBdaMv3sknVzsODNbBAwnuP/eBR4ys+mSLpQ0MMruI6kCGAzcIml64rzdCC24F7JE3yvpLeAtoD1wUTFdHMdxnPpHGhfiMUBfM/seQNJlwCTg+mIHmtlEYGJW2HmJ7ckE12KuY2eRY9CHmTWcqZYdx3GcvKQxYAIWJ/YXk3uAhlMEn9necRyn6khjwG4HXpU0Nu7vC4wqn0qO4ziOU5yiBszMrpb0PGE4vYCjzeyNcivmOI7jOIXIa8AkrWZm38apnGbFXyaunZnNLb96juM4jpObQi2w+4C9CXMgJr+jUtxft4x6OY7jOE5B8howM9s7/vvMl47jOE6tI813YM+mCXMcx3Gc6qRQH1gLoBXQXlJblg2dXw1Yuxp0cxzHcZy8FOoDOx44jWCsprLMgH1LWOfLcRzHcWqMQn1g1wLXSjrZzIrOuuE4juM41Uma78Cul7QJ0BNokQi/q5yKOY7jOE4hihowSecDOxEM2ETC8iovA27AHMdxnBojzYKWBwK7Av8zs6OBzQFfBdlxHMepUdIYsB/MbAmwSNJqwJf4R8yO4zhODZPGgE2RtDpwK2E04uvAa2mESxog6T1JMySdlSO+n6TXJS2SdGBW3GJJ0+JvfCK8u6RXJb0v6cG4WKbjOI7TwEizIvNJZvaNmd0M9AeGRldiQSQ1Jgy335PQf3aIpJ5ZyT4GjiJMW5XND2a2RfwNTIRfBlxjZj2ArwnrlTmO4zgNjEIfMm9VKM7MXi8iextghpnNjMc8AAwC3skkiItWImlJGmUlCdgFODQG3QmMAG5Kc7zjOI5Tfyg0CvGqAnFGMCSF6AR8ktivAPqm1AughaQpwCLgUjMbB6wBfGNmixIyV1i1GUDSMGAYQJcuXUo4bd3BF8h0HKchU+hD5p1XUnauVZstR1g+upjZZ5LWBZ6T9BZhFpBUMs1sJDASoHfv3qWc13Ecx6kDpPkO7Mhc4Sk+ZK4A1knsdwY+S6uYmX0W/2fGBTW3BB4BVpfUJLbCSpLpOI7j1B/SjELsk/j9itDnNLDQAZHJQI84arAZMAQYX+QYACS1ldQ8brcHfgm8Y2YG/JPwbRrAUOCxNDIdx3Gc+kWaqaROTu5LagPcneK4RZKGA08BjYHRZjZd0oXAFDMbL6kPMBZoC+wj6QIz6wVsDNwSB3c0IvSBZQZ//AF4QNJFwBvAqLQX6ziO49QfihqwHCwAeqRJaGYTCdNPJcPOS2xPJrgBs4/7N7BpHpkzCSMcHcdxnAZMmj6wCSwbKNGY0Dp6qJxKOY7jOE4x0rTArkxsLwI+MrOKMunjOI7jOKlIMxPHC8B7QBugHcGIOY7jOE6NUtSASTqWMPfh/oTRf69I+k25FXMcx3GcQqRxIZ4BbGlmcwAkrQH8GxhdTsUcx2mA+PQyTgmk+Q6sAvgusf8dy08R5TiO4zjVTpoW2KfAq5IeI4xGHAS8Jul0ADO7uoz6OY7jOE5O0hiwD+IvQ2bmi9ZVr47jOI7jpCPNTBwXAEhqHXZtftm1chzHcZwipBmFuImkN4C3gemSpkrqVX7VHMdxHCc/aQZxjARON7OuZtYV+B1wa3nVchzHcZzCpDFgq5jZPzM7ZvY8sErZNHIcx3GcFKQZxDFT0p9YNgP94cCH5VPJcRzHcYqTpgX2G6AD8Gj8tQeOTiNc0gBJ70maIemsHPH9JL0uaZGkAxPhW0iaJGm6pDclHZyIu0PSh5Kmxd8WaXRxHMdx6hcFW2CSOgBdgfPM7JtSBEtqDNwI9Cd8DD1Z0vjEul4AHwNHAb/POnwBcKSZvS9pbWCqpKcSOpxhZmNK0cdxHMepX+Q1YHEOxEsI34B1lzTMzFKtqBzZBpgR1+9C0gOEj6CXGjAzmxXjliQPNLP/JrY/k/QloRVYkhF10uMz+DiOU9co5EI8DehlZtsB2wNnlyi7E8tPOVURw0pC0jZAM5b/mPri6Fq8RlLzPMcNkzRF0pTZs2eXelrHcRynllPIgP1sZrNh6SrIOQ1FAZQjzHKE5RcgdSQMHjnazDKttLOBjYA+hOVd/pDrWDMbaWa9zax3hw4dSjmt4ziOUwco1AfWWdJ1+fbN7JQisiuAdZLHA5+lVUzSasATwLlm9krivJ/HzZ8k3c6K/WeO4zi1G/fZVwmFDNgZWftTS5Q9GeghqTthQuAhwKFpDpTUDBgL3GVmD2fFdTSzzyUJ2JcwQ4jjOI7TwMhrwMzszpURbGaLJA0HngIaA6PNbLqkC4EpZjZeUh+CoWoL7CPpAjPrBRwE9APWkHRUFHmUmU0D7o2jIwVMA05YGT0dx3GcukmaD5krjZlNBCZmhZ2X2J5McC1mH3cPcE8embtUsZqO4zhOHSTNh8yO4ziOU+vIa8AkXRb/B1efOo7jOI6TjkIuxL0knUsYtv5wgXSO49QlfAScU08oZMCeBL4CVpH0LWHQhGX+zWy1atDPcRzHcXKS14VoZmeYWRvgCTNbzcxaJ/+rUUfHcRzHWYGioxDNbJCktQgzXwC8mpmhw3Ecx3FqiqKjEOMgjteAwYTvs15LLn3iOI7jODVBmu/AzgX6mNmXsHSJlWcAX87EcRzHqTHSfAfWKGO8InNSHuc4juM4ZSNNC+xJSU8B98f9g8maXcNxHMdxqps0gzjOkLQ/sANhCP1IMxtbds0cx3EcpwCp5kI0s0eBR8usi+M4juOkxvuyHMdxnDpJWQ2YpAGS3pM0Q9JZOeL7SXpd0qLsofmShkp6P/6GJsK3lvRWlHldXBfMcRzHaWCkMmCSWkrasBTBkhoDNwJ7Aj2BQyT1zEr2MXAUcF/Wse2A84G+wDbA+ZLaxuibgGFAj/gbUIpejuM4Tv0gzYfM+xAWjnwy7m8haXwK2dsAM8xsppn9DDwADEomMLNZZvYmsCTr2D2Af5jZXDP7GvgHMEBSR2A1M5tkZgbcRViV2XEcx2lgpGmBjSAYo28A4qrI3VIc1wn4JLFfEcN75cVxAAAgAElEQVTSkO/YTnG7qExJwyRNkTRl9myf+cpxHKe+kcaALTKzeZWQnatvylby2NQyzWykmfU2s94dOnRIeVrHcRynrpDGgL0t6VCgsaQekq4H/p3iuApgncR+Z+CzlHrlO7YibldGpuM4jlOPSGPATgZ6AT8RZuP4FjgtxXGTgR6SuktqBgwB0vSdATwF7C6pbRy8sTvwlJl9Dnwnads4+vBI4LGUMh3HcZx6RJqZOBYA58RfasxskaThBGPUGBhtZtMlXQhMMbPxkvoAY4G2wD6SLjCzXmY2V9KfCUYQ4EIzmxu3TwTuAFoCf48/x3Ecp4FR1IBJmsCK/UzzgCnALWb2Y75jzWwiWfMmmtl5ie3JLO8STKYbDYzOET4F2KSY3o7jOE79Jo0LcSYwH7g1/r4FvgA2iPuO4ziOU+2kmQtxSzPrl9ifIOlFM+snaXq5FHMcx3GcQqRpgXWQ1CWzE7fbx92fy6KV4ziO4xQhTQvsd8DLkj4gfIfVHThJ0irAneVUznEcx3HykWYU4kRJPYCNCAbsP4mBG38tp3KO4ziOk49U64ERJs3dEGgBbCYJM7urfGo5Tj1gn31yh0+YUL16OPWfBprX0gyjPx/YiTCj/ETC7PIvEybSdRzHcZwaIU0L7EBgc+ANMzta0lrAbeVVy3Ecp5aRr5UD9b6lU1tJMwrxBzNbAiyStBrwJbBuedVyHMdxnMKkaYFNkbQ64aPlqYSPml8rq1aO4ziOU4Q0oxBPips3S3qSsKDkm+VVy3Ecx3EKk2YQx7NmtiuEFZSzwxynRvF+CcdpsOQ1YJJaAK2A9nFJk8xikqsBa1eDbo7jOI6Tl0ItsOMJ636tTej7yhiwb4Eb0wiXNAC4lrCcym1mdmlWfHPCcPytgTnAwWY2S9JhwBmJpJsBW5nZNEnPAx2BH2Lc7mb2ZRp9nOqjgX6W4jhONZLXgJnZtcC1kk42s+tLFSypMcHQ9SespDxZ0ngzeyeR7BjgazNbX9IQ4DKCEbsXuDfK2RR4zMymJY47LC6r4jQACnoJq08Nx3FqGWkGcVwvaXugWzJ9ipk4tgFmmNlMAEkPAIOApAEbBIyI22OAGyTJzJLrjx1CWAnacRzHcZaSZhDH3cB6wDRgcQw2is/E0Qn4JLFfAfTNlyau4DwPWAP4KpHmYIKhS3K7pMXAI8BFWQYvo/cwYBhAly5dsqMdx3GcOk6a78B6Az1zGYkiKEdYtoyCaST1BRaY2duJ+MPM7FNJrQkG7AhyGFMzGwmMBOjdu3epujuO4zi1nDQzcbwN/KISsiuAdRL7nYHP8qWR1ARoA8xNxA8hy31oZp/G/++A+wiuSsdxHKeBkaYF1h54R9JrwE+ZQDMbWOS4yUAPSd2BTwnG6NCsNOOBocAkwpyLz2VaepIaAYOBpatBRyO3upl9JakpsDfwTIprcBzHceoZaQzYiMoIjn1aw4GnCMPoR5vZdEkXAlPMbDwwCrhb0gxCy2tIQkQ/oCIzCCTSHHgqGq/GBON1a2X0cxzHceo2aUYhviCpK9DDzJ6R1IpgPIpiZhMJS7Akw85LbP9IaGXlOvZ5YNussO8J34w5juM4DZyifWCSjiMMcb8lBnUCxpVTKcdxHMcpRppBHL8FfkmYgQMzex9Ys5xKOY7jOE4x0hiwn8zs58xOHEjhw9Idx3GcGiWNAXtB0h+BlpL6Aw/jM/g4juM4NUwaA3YWMBt4izDB70Tg3HIq5TiO4zjFSDOMviVhCPytsHSS3pbAgnIq5jiO4ziFSNMCe5ZgsDK0xD8edhzHcWqYNAashZnNz+zE7VblU8lxHMdxipPGgH0vaavMjqStWbaYpOM4juPUCGn6wE4FHpaUmYi3I2GJE8dxHMepMQoasDihbjNgI2BDwvIn/zGzhdWgm+M4juPkpaABM7Mlkq4ys+0Iy6o4juM4Tq0gTR/Y05IOkJRr8UnHcRzHqRHS9IGdDqwCLJb0A8GNaGa2WrEDJQ0AriXMXn+bmV2aFd+csJry1sAc4GAzmyWpG/Au8F5M+oqZnRCP2Rq4gzCcfyJwaiVWi3acFdlnn/xxE3zyGcepbRRtgZlZazNrZGZNzWy1uJ/GeDUGbgT2BHoCh0jqmZXsGOBrM1sfuAa4LBH3gZltEX8nJMJvAoYBPeJvQDFdHMdxnPpHmuVUJOlwSX+K++tI2iaF7G2AGWY2M04G/AAwKCvNIODOuD0G2LWQq1JSR2A1M5sUW113Afum0MVxHMepZ6TpA/sbsB1waNyfT2hZFaMT8ElivyKG5UxjZouAecAaMa67pDckvSDpV4n0FUVkAiBpmKQpkqbMnj07hbqO4zhOXSKNAetrZr8FfgQws68JQ+uLkaslld1XlS/N50AXM9uS0Ad3n6TVUsok6jnSzHqbWe8OHTqkUNdxHMepS6QxYAtjf5YBSOoALElxXAWwTmK/M/BZvjRxnbE2wFwz+8nM5gCY2VTgA2CDmL5zEZmO4zhOAyCNAbsOGAusKeli4GXgkhTHTQZ6SOouqRkwBBiflWY8MDRuHwg8Z2YmqUM0mkhalzBYY6aZfQ58J2nb2Fd2JPBYCl0cx3GcekbRYfRmdq+kqcCuBBfevmb2borjFkkaDjxFGEY/2symS7oQmGJm44FRwN2SZgBzCUYOoB9woaRFwGLgBDObG+NOZNkw+r/Hn+M4jtPAyGvAJLUATgDWJyxmeUscaJEaM5tI+FYrGXZeYvtHYHCO4x4BHskjcwqwSSl6lA3/bshxHKfGKNQCuxNYCLxE+JZrY+C06lDKcRxnpfEKZr2nkAHraWabAkgaBbxWPSo5juM4TnEKDeJYOuN8qa5Dx3Ecxyk3hVpgm0v6Nm4LaBn3U8+F6DiO4zjlIq8BM7PG1amI41QHBbtFqk8Nx3GqgDTfgTmO4zhOrcMNmOM4jlMncQPmOI7j1EncgDmO4zh1kjQrMjt1kXyjFfwDTsdx6gluwJzqxWdHcByninAD5jgrgQ/Ld5yaw/vAHMdxnDpJWQ2YpAGS3pM0Q9JZOeKbS3owxr8qqVsM7y9pqqS34v8uiWOejzKnxd+a5bwGx3Ecp3ZSNhdiXJDyRqA/YSXlyZLGm9k7iWTHAF+b2fqShgCXAQcDXwH7mNlnkjYhrCnWKXHcYXFZFcdxHKeBUs4+sG2AGWY2E0DSA8AgIGnABgEj4vYY4AZJMrM3EmmmAy0kNTezn8qor+PUKD6+xXFKo5wGrBPwSWK/AuibL01cwXkesAahBZbhAOCNLON1u6TFhEUvLzIzyz65pGHAMIAuXbqs5KU4jpMX/2TDqSHKacCUIyzb0BRMI6kXwa24eyL+MDP7VFJrggE7ArhrBSFmI4GRAL17917BwDlOfSevXaleNRynbJRzEEcFsE5ivzPwWb40kpoAbYC5cb8zMBY40sw+yBxgZp/G/++A+wiuSsdxHKeBUc4W2GSgh6TuwKfAEODQrDTjgaHAJOBA4DkzM0mrA08AZ5vZvzKJo5Fb3cy+ktQU2Bt4pozX4DgO/r2bUzspmwGLfVrDCSMIGwOjzWy6pAuBKWY2HhgF3C1pBqHlNSQePhxYH/iTpD/FsN2B74GnovFqTDBet5brGpyIjy5wHKcWUtaZOMxsIjAxK+y8xPaPwOAcx10EXJRH7NZVqaPjONWLt+acqsKnknKcuoC3gh1nBXwqKcdxHKdO4gbMcRzHqZO4C7G24C4ix3GckvAWmOM4jlMncQPmOI7j1EncgDmO4zh1Eu8DcxynTuNzPjZcvAXmOI7j1EncgDmO4zh1EjdgjuM4Tp3E+8Acx3EiPk9j3cINmOM4ThmoamPoxnVFyupClDRA0nuSZkg6K0d8c0kPxvhXJXVLxJ0dw9+TtEdamY7jOE7DoGwGTFJj4EZgT6AncIiknlnJjgG+NrP1gWuAy+KxPQlrg/UCBgB/k9Q4pUzHcRynAVDOFtg2wAwzm2lmPwMPAIOy0gwC7ozbY4BdJSmGP2BmP5nZh8CMKC+NTMdxHKcBUM4+sE7AJ4n9CqBvvjRxBed5wBox/JWsYzvF7WIyAZA0DBgWd+dLeq8S11AQLb/bHvgqocDKyFpeXomycsgrj6xKyKu266yEPH8GpcvKIa/hXWcl5NXmZ5BF15UVUE7KacBy3TlLmSZfeK4WY7bMEGg2EhhZSMGqRNIUM+tdG+XVVllVLa+h6ObXWfPyGpJutZlyuhArgHUS+52Bz/KlkdQEaAPMLXBsGpmO4zhOA6CcBmwy0ENSd0nNCIMyxmelGQ8MjdsHAs+ZmcXwIXGUYnegB/BaSpmO4zhOA6BsLsTYpzUceApoDIw2s+mSLgSmmNl4YBRwt6QZhJbXkHjsdEkPAe8Ai4DfmtligFwyy3UNJVLV7sqqlFdbZVW1vIaim19nzctrSLrVWhQaPI7jOI5Tt/C5EB3HcZw6iRswx3Ecp07iBqwSxI+tq0JO46qQUw4krR5HhjopiQOLnFpCVb2nVYWkFlUsr1Z/o1UduAFLiaS2kk6WtCHQsgrkHUEYxLLSSGon6c+S9pbUIYZV6uWV1ELSvsBFwKZVoFtfSeuvrJyEvPUkrRq3Vyr/SmpVNVqBpDOBMZJOlNSp6AHF5XWSdJuk5lUga1VJu8ftlS7UJW0paWhVVMCibtuurJwoq4+kqyUdBmAr0cEvaStJv5e0WRXpdhFwUlVUCiWtHcuP06tKv7qKG7AUSPod8DSwK3Ae8MeVkHWspH7Ak8COkrZaSd1OAZ4HOgAHAI9A5V5eSX8A7id8xd8K6C2pdSX12kbS48BVhIJ9kKRKG35JW0t6EhgN/EPS2ma2ZCXknQ68IOkSSQXm+U6l14eEGQtuAfYAjlqJ+7aVpJuAjYGmwHGV1S3K2x3oDTwsac2VLNQ3i0a/F7AdsFKGJxrnXYEHVqYyIqmlpJHA3wgz9fxO0lUxriS5ktpI+htwM7A+cIWkkyupVztJI2Kl90WgP7BeZWRFeW0l3QpcQHhHFwLb12ZPTtkxM//l+RE+M9ifkFHWimE7Ak8AvUqUNZCQiR8C1olh5wITK6nbZoTPDl4FBifCPwUGlChrb+Bd4FGgUwwbRGghbl+irFaEwnce8McYdhJhzsuulbzWgwgfrP827t8GPBq3VaKsjYDphKHG2wKnAn/PPJMS5HQhVBr6EgqT1RK6PgJ0KFFeW+Am4A3g0Bi2F/B4Ze5bfKYvJe7T7cANlbz/7YBrYx7ZIl73hcBZQJtKyBsIvAD8kvA5zP3AeZXUbVfgeOA5YOfEM/4GaFcJeSOAqUDzuP9rwjeorUqUcyYwBbgUaBzDbo33rWUl9Pq/eE3nAk1i2BGESdB3qMy9qw8/b4HlQNLGku4A9jazRwkFXmbOxY+B+cC3Jcg7iJDRbjezg8wsM//jRUDnGJ/KvSNpQ0ljgYsJBvHlKKN9TDIB+DmlXp0k3Ut42SqAD83s06jbY8B3QD9Ja6WUdzowljAX283E7wzN7G+EGnuHNHIS8k6TtC4wm2Bc34pRvyfUPDtZfJNTyNokunGWAGsBV5jZK1Hu18AmKeW0knQloQW9jpm9SjD0N8QkUwgu5gVp5EWZfyQ8yyOA4WZ2X4yaCrwNDC9B1saSHgHuBf5hZvvHqD8Ae0naOq2sKG8nQp7/GtjMzKaZ2eyob2dChS6trG6SHiNUaP5qZv+y8H3nNcBgSesUlrCcrL0lvUpYreIfhPdga0lNzew/BK9E2nw7UNL4eG/uIXyTukWM/pSQ71K7/iSdBhwLnGJmZ8VrhGDM+gGpvS6xRf4SocL6LTDTzBbF6KcI+WwHSW3SyqxPuAFLoDDzx1WEl3+amY2LURcD50taHTiBkJm/KSKrsaT9Ja1HmEHk38BUhcERN0k6NSa9CDhLUvNChXGi4JxOyMT7mNlnhBbdFsCvJF1BWGbmv0V0axJdS/sAj5hZP4L7cdes/oj7CO6irQoZ11iYTAN2AI4zs5cIhmFLSZtL6gu8T3Ly08L67SfpGWBnwkv7b0JBslN0ze0ETAS+TCGrlaSrgbuAOWb2X+CvhBYFhAKgDeGj+WKy9orpfgZ+aWavx6jrgE0k3UBoTTwLLChWIYlu1l0IrbkdCXlhcKYyYmZfECoEG0r6ZRFZjaJ+Ewit8rOB5pJWi7K+JLTC/lzsOqO8PnFzLmFi7XvNbGF0Be9PaPF8CvSR1KWIrIyLa0fgV2Y2wMzGSmotqYuZvQY8k0Y3ST2jgR4PnG9mZ5jZzKhjR+A4SQ8CPwEfFpG1jqQngNOAm8xsqpnNIBjnodG9fz6hwvpdCr1OVOiffQ0YByyJFc5HJR1sZh8Q7ttQSe1SyDuK8P6daWZHA/sB10lqCkuf6cvxulNXJOoVNd0ErE0/gkvudWCbRNja8X8c8AVhzbLWKWStRTAAGbfXbwmFy7uEGmfrRNqnWeZuW8ElRsiczwFXA38CHorhjeL/mcCbBDdd0xS6/YJgpE/MCj8DeCIr7FxCzXG9HHLaEwzJy4Qlb36ViGtDaCn9j1A47ZLyGexPaCXtnRW+KaFV93q8F/1SyBpOqD3PzHGtbxEK9FcJbp0mue59TNs9/u8MfJoI3ztzXfH5fg+sW0SnzOQBHQjuwTMTcR3ite2TeLarAacDdxaQeXJ8nl2AVWNYb+B64OistG8CB+XKawnd2hNq9yfF/RNiHn2U0LLZNoZvQ6gMDC3yDB4F1oz7kwjTxx1NqNRk3o/OhBbnTnnkNCZUsiYBhxIM+xWJ+KaEVuZ/KeKOTNzbU4F/Ap0T93pNQgv6ySjrlCKymhMqMNPjc8rIOj0+y/8AJyTSr0JoMR6QK78l5L0bn2vXzPXH/8cJxjaTvhmhT/7PQJc071h9+tW4AjX9ixn2PGB7oDWhf2UgoQ9oMvD7mG5jQo0u4xtvkkPWLsBglu8PuTkWJq0IBuZPifQZWVsAHwBrZMlbi1A7PDrxYjQl9G0clkjXkeD6GJAIyy6csnUbTOj03jGRpg2h9nloIqxr1HufRFgjQsH0a2D/GDaERP9eDNuAYOAOiPuN8zyDjLy28ZrvBPrHuCsIbjUIC6COBTbKHJdHXkdC/9Hfga3js7weWD+RZk9CC26rAnmjO2HNueeAbjFsTMwjownuwoyeqxAK+cFxv1kOeS2JfacEgzkwHtMukeZ44EFixSmR9+4FDsyStyGhAB5LGCCwSta5jiYMLOmWCD+IUAlolFK3NgTD+g9Caz37mk4iVHB6FtBtN2KFjdB6nk1oQW2Ydcxwgtsz+xynApfEZ9oi8W58m/VMNycU/nvnegdi2CkEd+9aUce/AEcSjMXbwH6J9+MxYoWwQF7LjCZeJXnOKHsUCQOYyRPxfI+Qoy8sW14iPKPH2vG610vE9SVUJAbmy8v19VfjCtTYhS+r0TQjFORnJV7cFwg19H2yjrmZ4ErJJ+tlQuthNMsKvEuBcwg1q31jgbJt4thMh+xdwF9y6HYToYW1euKYAwluCiXCjiF0EvctUbezgI6J9AcQXHaNEmG/A/4Wt0VYs+1O4LKscz1GKIAzL1tz4JD4srYt8CzaEgroixM6vEmo1V5BHCgArEsocH5Ljk51gsF5kNDi65QI70VoOZ+Zlf7pTAGTeA6ZAqg/oaA/jVCxycR3AmYBV+Y4/68JfYktcsS1IQzQ+JLQcstc003ApTnu43GJ+9iK0KdyOaElknmmwygwMIPgTv4LcEZW+AvAWYnnmU+3m7OeyRigfdzP6NaDUODukZXf8uoG3A1clLnviXu+KqHSMSTub0zIi08QCukOMTxjxC4Hns2SfSJh1fY+WeEZWRMIg3cy13FUDH+ZLC9DzEtnZT9PQkv8dELFazRwduJ+b5mRQ6jUXU0cCMXy79TrwK9TyNuCMBqyeeLYy4Gns3S6gtjSI48noT7+GlQfWKZPQtJfgOslbWxhZef7CK2F/hYmGZ5KGHAxIabPfKD6B0JHceeEzB6EAhNCwfomoeA+T9JxhBbIL4DdLPSpLSD0V60FYdLjeOyPwH9z6HZP1K1f4lLGElqD5yXC7iNMfLywRN06svxw6HEEV9g5ibCuQCdJR1hgDqEW20fSFol0NxJcgOvFa/sJ+BfwA7B7Ih2SdpB0eEz3NaEGub2kXvH6pgCTLPRxzJPUyEJfxzuETvAeUU7mmfYnGMpJhL6CzzPxFiZ8fgVYP6uP7wzgEknrJJ5D5huu9QiF41/N7DvAog6fEp5Jpyh/aee+mT1BGOKshF47SzrdzOYRCuI5hIrM7ZJ+Qah0bCVpy4ReNxAK1g2j3AWEmvf3BCP+bEzXGGgkqaOkQyQdHge+dI/HvUNwkW4g6VcJ+X+J92Jn4P8K6DYS2EbSpoSCv4LQgsHMFsb/9wkGfo2Y3/LpdpjCd1VtCBWywyRtlrjvmNl8gututRi0C/Cwmf3azF61MHgEQv8WZnYm0FPSrxPX9mzUc068/6tkydrHzF4xs0x/7BMEA3afhT6q5Pt+K6Hy0zSGd5N0O8FY/MfCZxx3AifEPtsbCQZrvKQbzOwBQn9pP0mtzWxJ7KtsROhXWyOFvGsI78NNift0JrBbfH4ZdgRWj/FGQ6GmLWh1/lg2RHw0oTXyd2Kzm1DTvozg/+9DqCXukTg2UwPPVbv+f0T3HaE/5R6CT38swXV1D6HvajXC0OFHSLhcCMN+r47x+XS7BNggcUxvYBrLu4dyuTXT6PbHLDndWVYLbxnP/x6wOKbdLMadC9yddb6RBMOacak0IjGkPMpbheDjz5b3J+CuuL0DoSXSJ+5navyrEV2IcT/jWj2B5ftEGmfptQ7BKF/M8i3XIxO6rg18E7d/Q2hZHAlcGX+PE4csx3ufeT65XFXdCH1sU1hW0+5CcKv1IxROowiDLU4Hbs06/i9EFxvBWD7EsmHib8b7syGhRT+PUBkZSXB730Z0jRIqHxcQ3Y/xGv9FaHGk0e13wC0xza8IFYFNEnr2JLSUt0yp2x2Ed+wcwvJJyWvejNDPnBn6fwux74/g+jwCOIzEZwUxbGaO+9+Y0C90dQFZhxM+EdiB4IX5dYGyoxvhvTwuR5mwG8HN2ZdQdrSL192D0J95DbEfNabvFZ9JKfK+IeHuTj6DuH9cPt3r86/GFai2Cw0v7ry4vSahZfK3mLluJLiMrgaOiGnOIXyEu8LghRyyuwHvJ2S/wbKO7pMJAxzmA8fGsI6V0O2q+NI1Sxx3C3BuFej2HcFFlXRxZAqAv8b9YwgF3whCQbQzYej5Q8R+g5huA4L7dfssPdLKe5jouiUYt1FFnmkhg/MYsY8qptmFYFQOLiDzNoL7q2U8/yzCiMXT47O4j1A4H0JoBa/QN0Luwi5jgM8kVBzWJgxMyXwb+BGwe8q83B14NyOXUPCvQXA1tidUSnZLpF81sb1RJXT7mGXGc7uV1O1+QiWuUdxuQ3BjHktoPY9l2YCPzMCOCkKFbjyhP3Jq1jn/A+yV2D+W4Ca9s4isF4B/J/LalSRczzmubRrL3IMnEbwVPfOkvZtQeWhGjkpvJeTdyfIV2IzbdYVKa0P61bgC1XqxwSVwetw+ntAx3ZhQ232AMMLqOkJttDvBlZPqA1dC4ZkZ8DEceD4R15dQCy00yCKNbtcAmyaOSfVBZErdkq3NXAVAc8LItD0I/TM3xZf0NEKLMtmqGcjyAwpKkfd/wJiYpmcsbAoNtChmcO4Edo1p2xIM3dYF5K1CcD+1jPvZgx0eJvYzAscUkJOrcOqRuO7DY1zneM0LiKP+8uWRHM/0jBzhHQgVoI1yxKmSuv1A1ijOFPmtkG6bZoWvSTCqyZbdxlGfziz7eDrTB/YuiVGtJPpDKyHrvwRvxnaEvJz342dCmfAFwZ06geX7spsTugq2IbhBx8S8lLnnuVrpJctL+wwayq/GFajWiw0Zam6icHoO+E3c3pngQllC7ERdSdn/KEVOCbr9gRJrXaXolqcA6Bn/B8cXqSvQIhZUTxJcgTldGJWUt4Q4/JvY2V7k2goZnAeTBUPK+3UCcEeO8IEE11vRWViKFE4HE1oaSbdSqTOBZJ5pi/jbiFDZeZM4QKMu6MayIe23sWwE532EATbdc8jegFCJ+EWB85ciawyxUpXy2q4nVrCywlsQ3NNvkhghXN3yGtqvxhWo9gtOFE6EodRTWdbf047QQulYSdnHE11esbC7Iis+51Dc2qZbngKga9x/APhD3G5McMu9RGzl5Dl3qfJeJuECK+WZZoVnDM4KrZEi8hoRXE09CTXhgYQKxD9JfOuWQk7OwinGPUFo/TROhKlYHsnxTG9O3INRpPcY1CrdCEZvYSywz8w696qEz1zOJ4zeO6fIuUuWRcqRe1H2bJaNhBRhgNSOZE0fRp7PRsopr6H9alyBar/gZYVT5puXBwgjDqtK9jek6DerzboVKQD6EkZ69Snxpa8yeQXu20oZnCyZ2wEvx+1fU4lacJ7C6ZR4rQVbliU+025Z97RRsXtZG3UjfLeW61uzloQRo/eQ3kBXmawcMo4nuKe3JwxquZ3EN5ylGpqqlteQfjWuQI1cdCicXonbGxI/xq0i2Zk+nozvO3WttTbplq8AiHF/JQzzTm1wqlpenvu2UgYnh8xJlDhpcw4ZxQqnkvJHDvlrZe2X2kqqNbqxrCKyftzvTegXHUjpbvMqk5VH9jeE1v1KT6Rb1fIa0q/GFaixCw/ffmxW03rUVt3yFAD3ElybJRdsVS0vzzlW2uBkyVvpmm9tLpxqo26xIvI6YcTqZAoMlKlOWTlkZ8+as7LGvkrlNZRfpibe4JDU2JbNEl2rqC26SdqOUEN/gjDr981mVulFOKtaXg75teK+ZSNpDQsff2f2G9lKrGVWldRG3ST9k/ApxhkWPoavFbLyyK/SPFdb83BtpcEaMCcdVV0AlLtAqc3U5sKpNulWlbrUputyqh43YE5BvIbpOE5txQ2Y4ziOUydpUJP5Oo7jOPUHN2CO4zhOnUYqp/sAAAPESURBVMQNmOM4jlMncQPmOI7j1EncgDn1GkmLJU2T9LakhyW1qmmdkkiaH//XljQmbm8haa8SZBwdr3GapJ8lvRW3Ly2X3o5TG/BRiE69RtJ8M1s1bt9LWEvq6pTHln3If1K/RNhRQG8zG14JebPisV8VS+s4dR1vgTkNiZeA9QEkHS7ptdhSuUVS4xg+X9KFkl4FtpN0qaR3JL0p6cqYpqukZ2PYs5K6xPA7JF0n6d+SZko6MIavGtO9HltHg7IVi8vVvx2Xs78QODjqdrCk9yV1iOkaSZohqX2xi5XUOKZtl9ifKamdpHsk3STpJUn/lbRnTNNE0tXx3rwp6diVv+2OUx7cgDkNAklNCPMuviVpY8K6V780sy0I65kdFpOuArxtZn2Bd4D9CPMrbgZcFNPcANwVw+4lLIKaoSNhifq9gYwL70fCqtVbEdZ2u0qSculpZj8D5wEPmtkWZvYgYeb0jH67Af8vTQsrth7vBw6NQXsAk81sbtxfh7Bsxz7ASEnNCYuDfmlm2xBmpv9txkA7Tm3DDZhT32kpaRowBfiYsC7VrsDWwOQYtyuwbky/mLDCNMC3BONzm6T9CasmQ5gk9r64fTfBYGUYZ2ZLzOwdYK0YJuASSW8SViTulIhLw2jgyLj9G8Ks8WkZBQzNc+xDUdf3gE8IqzLvDhwd78urwOox3HFqHU1qWgHHKTM/xFbWUmLr504zOztH+h8z/V5mtkjSNgQDN4SwoOguOY5JdiQn53fMtLIOIyxjv7WZLYz9VC3SXoCZfSLpC0m7ENZPO6zYMYljZ0n6WtLOwJaExURz6Z3ZF3CSmT2b9hyOU1N4C8xpiDwLHChpTYDYJ9Q1O5GkVQkrYk8ETgMyhvDfBIMGwZi8XOR8bQhuuYXRkKxwriy+A1pnhd1GcCU+VImBJaMIrs4HsmaaH6zA/2/XDlUiCqI4jH8nCvoaCuYNFoNvYbEKblBsgnajUWwGbYLiWtQgmg0iBkHf5RhmFpYtlyuy7OD3i4cZ5rb/3HNmmdJO/ALugWFtuRIRKxGx0PM8aSYMMP07tb13BDzUtt4jZXY1bQm4q2uegf1a36W02d6BLWCv48hLYBARr5TA++xY/wSsjh9x1NotsEi/9uHYNSVEz6fq38ALMAK26/ztjBJkbxHxAZxip0Zzymf0UgMiYgCcZOb6L/auAceZuTFRuwCuMvPmDz9TmilvVtKci4gDYIces6+JvYeUl4WbXWul1vgHJklqkjMwSVKTDDBJUpMMMElSkwwwSVKTDDBJUpN+ANvdvD1tu19FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import style \n",
    "\n",
    "fig, ax = plt.subplots(1) \n",
    "fig.autofmt_xdate()\n",
    "index = np.arange(16)\n",
    "bar_width = 7\n",
    "opacity = 0.7\n",
    "\n",
    "kaggle_bar = plt.bar(index*20, kaggle, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 label='Kaggle')\n",
    "actual_bar = plt.bar(index*20 + bar_width, actual, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 label='Actual')\n",
    "\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Percentage of Population')\n",
    "plt.title('Comparing Personality Distribution of Kaggle Dataset to the World')\n",
    "plt.xticks(index*20 + bar_width, keys)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above compares the percentage of people who belong to each type on the Kaggle database and Myers-Briggs world estimates.  If the data weren't already skewed enough, the people's personalities of this dataset are quite different to the general population.  When it comes to creating a system for understanding personality, the data used to train the model would ideally represent the population of personalities it is trying to understand.\n",
    "\n",
    "Insert Facets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "Preprocessing the data will be the largest part of the technique for predicting the personality of a person based on their text.  There are separating characters (|||) that need to be removed, links that will not show up in language and instead will be converted simply into the word 'link'.  \n",
    "\n",
    "As mentioned in the Data Exploration section above, using the bag of words technique to create word vectors of each personality type's language. Using spaCy's medium web language library to create relative frequencies of common language used online, the result will be a model of language use specific to each personality feature.  In addition, spaCy can find the similarities of new sentences to each corresponding personality and even between personalities.  \n",
    "\n",
    "The classifier is a Deep Neural Network, that will run over the language and assign the probability for each class based on their word vector representations.\n",
    "\n",
    "Since the data is quite skewed not only from the population but especially not distributed evenly throughout the personality types, creating a model that will most accurately predict the personality type will likely result in simply guessing the most likely personality features, in this case Introverted and Intuitive (IN).  Tuning the model based on the AUC and series of 4 binary classifications (which coincidentally also makes more sense in the study of personality) allows to create the most potentially useful prediction model and avoid overfitting.  \n",
    "\n",
    "Calculating AUC not only tries to get the most predictions right, but tries to ensure that false positives and false negatives don't crop up.  In other words, since the accuracy metric would tend to create a prediction where everyone is IN__, the AUC metric will catch that the model systematically falsely assigns Extroverts the Introvert label and will look for patterns to help correct it, even if retroactively.  It will be used as an evaluation of the quality of the models and a window into how much information was actually learned.\n",
    "\n",
    "I also have read that AUC on skewed datasets is overused and doesn't necessarily represent a good way to compare models. To compensate, I added F1_score which was suggested on a few forums. https://stats.stackexchange.com/questions/210700/how-to-choose-between-roc-auc-and-f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "The Benchmark I will be using to predict personality based on the data is a Logistic Regression, Random Forrest Classifier, and Support Vector Machine Classification algorithms, as I am trying to make a similar model to Yilun Wang's in his Understanding Personality through Social Media.\n",
    "\n",
    "The average AUC across the four binary predictions he achieved with his best model was 0.661 using a large repository of Tweets of around 90,000 twitter users.  Of course this dataset is much more skewed and smaller.  I used another top kernel from kaggle with this same dataset where F1_score was used. His best models created an average F1_score of 0.665.\n",
    "See links for these projects in the conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "First, we must do a little clean up of the data. Below shows the posts being split where '|||'s are, and saved into an array of posts.  Then save the posts and type to a list (keep_list and type_list respectively) to save into a DataFrame called frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "keep_list = []\n",
    "type_list = []\n",
    "\n",
    "for i in range(len(data.type)): \n",
    "    string = data['posts'][i]\n",
    "    temp = np.array(string.split('|||'))\n",
    "    keep_list.append(temp)\n",
    "    type_list.append(df['type'][i])\n",
    "    if i%2000 == 0:\n",
    "        print(i)\n",
    "\n",
    "frame = pd.DataFrame({\n",
    "    'posts' : keep_list,\n",
    "    'type'  : type_list\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove the hyperlinks and replace them with the word link. I figured this would act as a way to preserve the information that at least there was a link involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  posts  type\n",
      "8670  ['link, IxFP just because I always think of ca...  ISFP\n",
      "8671  ['So...if this thread already exists someplace...  ENFP\n",
      "8672  ['So many questions when i do these things.  I...  INTP\n",
      "8673  ['I am very conflicted right now when it comes...  INFP\n",
      "8674  ['It has been too long since I have been on pe...  INFP\n"
     ]
    }
   ],
   "source": [
    "# Go through each post to remove links\n",
    "import re\n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "for posts in frame.posts:\n",
    "    for i in range(len(posts)):\n",
    "        posts[i] = pattern.sub('link', posts[i])\n",
    "    \n",
    "print(frame.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the more difficult prediction of 1 of 16 MBTI personality types, I want to give each prediction class a number rather than the string. (I did this because I ran into some issues with keras trying to predict the string and put it through to_categorical())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               posts  type  label  e  n  t  p\n",
      "0  ['link, link, enfp and intj moments  link  spo...  INFJ     13  0  1  0  0\n",
      "1  ['I'm finding the lack of me in these posts ve...  ENTP      0  1  1  1  1\n",
      "2  ['Good one  _____   link, Of course, to which ...  INTP      1  0  1  1  1\n",
      "3  ['Dear INTP,   I enjoyed our conversation the ...  INTJ      9  0  1  1  0\n",
      "4  ['You're fired., That's another silly misconce...  ENTJ      8  1  1  1  0\n"
     ]
    }
   ],
   "source": [
    "def type_to_numbers(type_list):\n",
    "    \n",
    "    label_list = []\n",
    "    for i in type_list:\n",
    "        if i == 'ENTP':\n",
    "            label_list.append(0)\n",
    "        if i == 'INTP':\n",
    "            label_list.append(1)\n",
    "        if i == 'ESTP':\n",
    "            label_list.append(2)\n",
    "        if i == 'ISTP':\n",
    "            label_list.append(3)\n",
    "        if i == 'ENFP':\n",
    "            label_list.append(4)\n",
    "        if i == 'INFP':\n",
    "            label_list.append(5)\n",
    "        if i == 'ESFP':\n",
    "            label_list.append(6)\n",
    "        if i == 'ISFP':\n",
    "            label_list.append(7)\n",
    "        if i == 'ENTJ':\n",
    "            label_list.append(8)\n",
    "        if i == 'INTJ':\n",
    "            label_list.append(9)\n",
    "        if i == 'ESTJ':\n",
    "            label_list.append(10)\n",
    "        if i == 'ISTJ':\n",
    "            label_list.append(11)\n",
    "        if i == 'ENFJ':\n",
    "            label_list.append(12)\n",
    "        if i == 'INFJ':\n",
    "            label_list.append(13)\n",
    "        if i == 'ESFJ':\n",
    "            label_list.append(14)\n",
    "        if i == 'ISFJ':\n",
    "            label_list.append(15)\n",
    "        \n",
    "    return label_list\n",
    "\n",
    "frame['label'] = type_to_numbers(type_list)\n",
    "    \n",
    "print(frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loop through each person and each post of each person, concatenate them into one string separated by commas, and run it through the SpaCy Natural Language Processing (NLP) model. Then save it to posts_vector_1d list.\n",
    "\n",
    "*I tried making tensors of a person's language by processing each post with SpaCy and saving each of 50 to a numpy array, but I haven't gotten it to work in keras yet. It also take about 4 hours to process on my CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n"
     ]
    }
   ],
   "source": [
    "#posts_vector_2d_list = []\n",
    "posts_vector_1d_list = []\n",
    "\n",
    "c=0\n",
    "for posts in frame.posts:\n",
    "    #p = np.empty((50,300))\n",
    "    all_posts = \"\"\n",
    "    for i in range(len(posts)): \n",
    "        string = posts[i].tostring()\n",
    "        s = string.decode('UTF-32')\n",
    "        all_posts += s + \". \"\n",
    "        #post_doc = nlp(s)\n",
    "        #p[i] = post_doc.vector\n",
    "    all_posts_doc = nlp(all_posts)\n",
    "    #posts_vector_2d_list.append(p)\n",
    "    posts_vector_1d_list.append(all_posts_doc.vector)\n",
    "    if c%1000==0:\n",
    "        print(c)\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can loop through each person's the MBTI type and break it into 0/1 for I/E, S/N, F/T, and J/P respectively so we can predict individual features and functions of personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\User\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\User\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "                                               posts  type  label  e  n  t  p\n",
      "0  ['link, link, enfp and intj moments  link  spo...  INFJ     13  0  1  0  0\n",
      "1  ['I'm finding the lack of me in these posts ve...  ENTP      0  1  1  1  1\n",
      "2  ['Good one  _____   link, Of course, to which ...  INTP      7  0  1  1  1\n",
      "3  ['Dear INTP,   I enjoyed our conversation the ...  INTJ      9  0  1  1  0\n",
      "4  ['You're fired., That's another silly misconce...  ENTJ      8  1  1  1  0\n"
     ]
    }
   ],
   "source": [
    "frame['e'] = 0\n",
    "frame['n'] = 0\n",
    "frame['t'] = 0\n",
    "frame['p'] = 0\n",
    "\n",
    "\n",
    "for i in range(len(type_list)): \n",
    "    if frame.type[i][0] == 'E':\n",
    "        frame['e'][i] = 1\n",
    "    if frame.type[i][1] == 'N':\n",
    "        frame['n'][i] = 1\n",
    "    if frame.type[i][2] == 'T':\n",
    "        frame['t'][i] = 1\n",
    "    if frame.type[i][3] == 'P':\n",
    "        frame['p'][i] = 1\n",
    "    if i%2000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The process for which metrics, algorithms, and techniques were implemented with the given datasets or input data has been thoroughly documented. Complications that occurred during the coding process are discussed.\n",
    "\n",
    "1. Preprocess the data, splitting the post string into an array of posts.\n",
    "2. Remove the hyperlinks and relace with the word link\n",
    "3. Process each post with SpaCy nlp library turning each post into an average word vector over the words of the post.\n",
    "4. Save a pd.df of all the posts and their respective person's type for simple classifiers\n",
    "5. Split the posts_df into train and test sets\n",
    "6. Train on Logistic Regression, Random Forrest, and SVC Classifiers\n",
    "7. Test accuracy, AUC, and F1 Score of different personality types and functions\n",
    "8. Train on SKLearn MLPClassifier and test Acc, AUC, and F1 of each personality type/ function\n",
    "9. Split persons_word_vector_df into train, valid, and test datasets\n",
    "10. Input into keras DNN and train/ validate to create weights for each personality type/ function\n",
    "11. Test accuracy, AUC, and F1 of keras models\n",
    "12. Train, (Validate), and Test Multilabel Classification algorithms for each type of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "Here I am implementing the benchmarks and Deep Learning algorithms. Any refinement beyond what you see here will be at the bottom labeled \"Anything below is just code from me messing around too much, learning, or otherwise getting off task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html\n",
    "#scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, roc_curve, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing all the libraries for the benchmark models, we enter into what will be a familiar cadence of steps:\n",
    "\n",
    "Split the data into train and test sets, then create and fit the models, followed by measuring for accuracy.\n",
    "\n",
    "### Benchmark for Extroversion vs. Introversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Accuracy:  0.7846934071000461\n",
      "AUC:  0.51472747716303\n",
      "F1 Score:  0.06786427145708582\n",
      "RandomForestClassifier\n",
      "Accuracy:  0.7699400645458737\n",
      "AUC:  0.5228257509274403\n",
      "F1 Score:  0.1381692573402418\n",
      "SVC\n",
      "Accuracy:  0.7819271553711388\n",
      "AUC:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, frame['e'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rnd_for = RandomForestClassifier()\n",
    "svm_clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "for model in (log_reg, rnd_for, svm_clf): #svm_clf,\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)    \n",
    "    print(model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc)\n",
    "    f1score = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for INtuition vs. Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Accuracy:  0.8575380359612724\n",
      "AUC:  0.49946294307196565\n",
      "F1 Score:  0.9233060312732689\n",
      "RandomForestClassifier\n",
      "Accuracy:  0.8409405255878285\n",
      "AUC:  0.5210790470825739\n",
      "F1 Score:  0.9125918419052446\n",
      "SVC\n",
      "Accuracy:  0.8584601198709082\n",
      "AUC:  0.5\n",
      "F1 Score:  0.923840238154304\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, frame['n'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rnd_for = RandomForestClassifier()\n",
    "svm_clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "for model in (log_reg, rnd_for, svm_clf): #svm_clf,\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)    \n",
    "    print(model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc)\n",
    "    f1score = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for Thinking vs. Feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Accuracy:  0.7584140156754264\n",
      "AUC:  0.7539748631351686\n",
      "F1 Score:  0.7265135699373694\n",
      "RandomForestClassifier\n",
      "Accuracy:  0.6685108344859382\n",
      "AUC:  0.6569473359549696\n",
      "F1 Score:  0.5907797381900968\n",
      "SVC\n",
      "Accuracy:  0.7736284001844168\n",
      "AUC:  0.7708844166859433\n",
      "F1 Score:  0.7488491048593351\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, frame['t'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rnd_for = RandomForestClassifier()\n",
    "svm_clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "for model in (log_reg, rnd_for, svm_clf): #svm_clf,\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)    \n",
    "    print(model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc)\n",
    "    f1score = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for Perceiving vs. Judging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Accuracy:  0.6454587367450438\n",
      "AUC:  0.5728658574786054\n",
      "F1 Score:  0.7559504919073311\n",
      "RandomForestClassifier\n",
      "Accuracy:  0.5859843245735362\n",
      "AUC:  0.5685350128072716\n",
      "F1 Score:  0.6564651874521805\n",
      "SVC\n",
      "Accuracy:  0.6256339326878746\n",
      "AUC:  0.5206361750380817\n",
      "F1 Score:  0.763953488372093\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, frame['p'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rnd_for = RandomForestClassifier()\n",
    "svm_clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "for model in (log_reg, rnd_for, svm_clf): #svm_clf,\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)    \n",
    "    print(model.__class__.__name__)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc)\n",
    "    f1score = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accruacy:  0.8063623789764869\n",
      "AUC:  0.6383506521999283\n",
      "F1 Score:  0.4339622641509434\n",
      "Accruacy:  0.8630705394190872\n",
      "AUC:  0.5380488214486892\n",
      "F1 Score:  0.925545249435949\n",
      "Accruacy:  0.7897648686030428\n",
      "AUC:  0.7879944482997918\n",
      "F1 Score:  0.7692307692307693\n",
      "Accruacy:  0.6809589672660212\n",
      "AUC:  0.647889977392615\n",
      "F1 Score:  0.7532097004279601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "for label in [frame['e'], frame['n'], frame['t'], frame['p']]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, label, test_size=0.25, random_state=42, shuffle=True)\n",
    "    simple_DNN = MLPClassifier()\n",
    "    simple_DNN.fit(X_train, y_train)\n",
    "    y_pred = simple_DNN.predict(X_test)\n",
    "    print(\"Accruacy: \", accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    print(\"AUC: \", metrics.auc(fpr, tpr))\n",
    "    print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is a drastic improvement in performance when the simple MultiLayer Perceptron Algorithm from sklearn over the baseline algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_434 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_435 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_436 (Dense)            (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_437 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_438 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 131,037\n",
      "Trainable params: 131,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, activation='relu', input_shape=(300,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4554 samples, validate on 1952 samples\n",
      "Epoch 1/25\n",
      " - 1s - loss: 0.3809 - acc: 0.8377 - val_loss: 0.4445 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44454, saving model to Extrovert_Introvert_DNN.weights.best.hdf5\n",
      "Epoch 2/25\n",
      " - 1s - loss: 0.3755 - acc: 0.8379 - val_loss: 0.4339 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44454 to 0.43393, saving model to Extrovert_Introvert_DNN.weights.best.hdf5\n",
      "Epoch 3/25\n",
      " - 1s - loss: 0.3736 - acc: 0.8386 - val_loss: 0.4354 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/25\n",
      " - 1s - loss: 0.3961 - acc: 0.8259 - val_loss: 0.4482 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/25\n",
      " - 1s - loss: 0.3801 - acc: 0.8340 - val_loss: 0.4644 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/25\n",
      " - 1s - loss: 0.3722 - acc: 0.8432 - val_loss: 0.4330 - val_acc: 0.8110\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43393 to 0.43305, saving model to Extrovert_Introvert_DNN.weights.best.hdf5\n",
      "Epoch 7/25\n",
      " - 1s - loss: 0.3976 - acc: 0.8252 - val_loss: 0.4941 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/25\n",
      " - 1s - loss: 0.3816 - acc: 0.8344 - val_loss: 0.4516 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/25\n",
      " - 1s - loss: 0.3719 - acc: 0.8347 - val_loss: 0.4357 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/25\n",
      " - 1s - loss: 0.3792 - acc: 0.8395 - val_loss: 0.4444 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/25\n",
      " - 1s - loss: 0.3855 - acc: 0.8320 - val_loss: 0.4359 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/25\n",
      " - 1s - loss: 0.3789 - acc: 0.8377 - val_loss: 0.4459 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/25\n",
      " - 1s - loss: 0.3764 - acc: 0.8417 - val_loss: 0.4372 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/25\n",
      " - 1s - loss: 0.3721 - acc: 0.8426 - val_loss: 0.4333 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/25\n",
      " - 1s - loss: 0.3660 - acc: 0.8463 - val_loss: 0.4340 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/25\n",
      " - 1s - loss: 0.3754 - acc: 0.8357 - val_loss: 0.4511 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/25\n",
      " - 1s - loss: 0.3611 - acc: 0.8463 - val_loss: 0.4747 - val_acc: 0.7894\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/25\n",
      " - 1s - loss: 0.3699 - acc: 0.8454 - val_loss: 0.4385 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/25\n",
      " - 1s - loss: 0.3643 - acc: 0.8412 - val_loss: 0.4389 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/25\n",
      " - 1s - loss: 0.3791 - acc: 0.8382 - val_loss: 0.4407 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/25\n",
      " - 1s - loss: 0.3833 - acc: 0.8340 - val_loss: 0.4375 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/25\n",
      " - 1s - loss: 0.3778 - acc: 0.8366 - val_loss: 0.4376 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/25\n",
      " - 1s - loss: 0.3647 - acc: 0.8428 - val_loss: 0.4382 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/25\n",
      " - 1s - loss: 0.3614 - acc: 0.8454 - val_loss: 0.4364 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/25\n",
      " - 1s - loss: 0.3675 - acc: 0.8412 - val_loss: 0.4365 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, frame['e'], test_size=0.25, random_state=1, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='Extrovert_Introvert_DNN.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "#hist = model.fit(X_train, y_train, batch_size = 80, epochs =20, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)\n",
    "hist = model.fit(X_train, y_train, batch_size = 75, epochs =25, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy:  0.8137390501436521\n",
      "AUC:  0.8061750849782338\n",
      "F1 Score:  0.5853211009174313\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# load the model weights that had the best validation score\n",
    "model.load_weights('Extrovert_Introvert_DNN.weights.best.hdf5')\n",
    "\n",
    "# evaluate the model's accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "# print the accuracy\n",
    "print('\\nTest accuracy: ', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"AUC: \", auc)\n",
    "#https://stackoverflow.com/questions/38015181/accuracy-score-valueerror-cant-handle-mix-of-binary-and-continuous\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, (y_pred>0.346).astype(int), pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4554 samples, validate on 1952 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.4537 - acc: 0.8445 - val_loss: 0.4010 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40104, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.4124 - acc: 0.8560 - val_loss: 0.3935 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40104 to 0.39349, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.4089 - acc: 0.8564 - val_loss: 0.3817 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39349 to 0.38168, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.4053 - acc: 0.8562 - val_loss: 0.4146 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3930 - acc: 0.8564 - val_loss: 0.3828 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3943 - acc: 0.8564 - val_loss: 0.3736 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38168 to 0.37364, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.3861 - acc: 0.8564 - val_loss: 0.3750 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.3818 - acc: 0.8562 - val_loss: 0.3827 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.3797 - acc: 0.8570 - val_loss: 0.3620 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.37364 to 0.36196, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.3768 - acc: 0.8575 - val_loss: 0.3750 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.3742 - acc: 0.8560 - val_loss: 0.3615 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36196 to 0.36146, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.3655 - acc: 0.8584 - val_loss: 0.3582 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.36146 to 0.35817, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.3679 - acc: 0.8557 - val_loss: 0.3823 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.3665 - acc: 0.8584 - val_loss: 0.3691 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.3639 - acc: 0.8597 - val_loss: 0.3871 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.3601 - acc: 0.8577 - val_loss: 0.3641 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.3584 - acc: 0.8630 - val_loss: 0.3803 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.3578 - acc: 0.8601 - val_loss: 0.3558 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.35817 to 0.35577, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.3608 - acc: 0.8614 - val_loss: 0.3616 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.3530 - acc: 0.8630 - val_loss: 0.3700 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.3517 - acc: 0.8647 - val_loss: 0.3538 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.35577 to 0.35379, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.3497 - acc: 0.8658 - val_loss: 0.3992 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.3519 - acc: 0.8643 - val_loss: 0.3528 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.35379 to 0.35284, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.3479 - acc: 0.8619 - val_loss: 0.3554 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.3503 - acc: 0.8634 - val_loss: 0.3501 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.35284 to 0.35015, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.3467 - acc: 0.8652 - val_loss: 0.3497 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.35015 to 0.34972, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.3469 - acc: 0.8643 - val_loss: 0.3542 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.3457 - acc: 0.8654 - val_loss: 0.3492 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34972 to 0.34922, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.3473 - acc: 0.8665 - val_loss: 0.3539 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.3519 - acc: 0.8634 - val_loss: 0.3530 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.3413 - acc: 0.8685 - val_loss: 0.3496 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.3451 - acc: 0.8636 - val_loss: 0.4054 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.3429 - acc: 0.8676 - val_loss: 0.3499 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.3415 - acc: 0.8676 - val_loss: 0.3597 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.3425 - acc: 0.8707 - val_loss: 0.3494 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.3386 - acc: 0.8647 - val_loss: 0.3531 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.3414 - acc: 0.8678 - val_loss: 0.3608 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.3369 - acc: 0.8702 - val_loss: 0.3490 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.34922 to 0.34896, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.3377 - acc: 0.8685 - val_loss: 0.4022 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.3387 - acc: 0.8702 - val_loss: 0.4096 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.3394 - acc: 0.8693 - val_loss: 0.3910 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.3389 - acc: 0.8669 - val_loss: 0.3468 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.34896 to 0.34679, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.3414 - acc: 0.8680 - val_loss: 0.3573 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.3346 - acc: 0.8704 - val_loss: 0.3608 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.3369 - acc: 0.8687 - val_loss: 0.3464 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.34679 to 0.34641, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.3364 - acc: 0.8691 - val_loss: 0.3462 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.34641 to 0.34618, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.3317 - acc: 0.8700 - val_loss: 0.3521 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.3345 - acc: 0.8711 - val_loss: 0.3534 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.3352 - acc: 0.8715 - val_loss: 0.3601 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.3374 - acc: 0.8676 - val_loss: 0.3460 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.34618 to 0.34600, saving model to Intuitive_Sensing_DNN.weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, frame['n'], test_size=0.25, random_state=1, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='Intuitive_Sensing_DNN.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "hist = model.fit(X_train, y_train, batch_size = 32, epochs =50, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy:  0.8810511753547021\n",
      "AUC:  0.7696554209099011\n",
      "F1 Score:  0.9354677338669335\n"
     ]
    }
   ],
   "source": [
    "# load the model weights that had the best validation score\n",
    "model.load_weights('Intuitive_Sensing_DNN.weights.best.hdf5')\n",
    "\n",
    "# evaluate the model's accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "# print the accuracy\n",
    "print('\\nTest accuracy: ', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, (y_pred>0.5).astype(int), pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_428 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_429 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_433 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 138,269\n",
      "Trainable params: 138,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4554 samples, validate on 1952 samples\n",
      "Epoch 1/64\n",
      " - 7s - loss: 0.6889 - acc: 0.5375 - val_loss: 0.6841 - val_acc: 0.5533\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68411, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 2/64\n",
      " - 1s - loss: 0.6695 - acc: 0.5863 - val_loss: 0.6380 - val_acc: 0.6255\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68411 to 0.63804, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 3/64\n",
      " - 1s - loss: 0.6194 - acc: 0.6693 - val_loss: 0.5999 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63804 to 0.59992, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 4/64\n",
      " - 1s - loss: 0.5782 - acc: 0.7051 - val_loss: 0.5630 - val_acc: 0.7131\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59992 to 0.56298, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 5/64\n",
      " - 1s - loss: 0.5604 - acc: 0.7148 - val_loss: 0.5420 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.56298 to 0.54196, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 6/64\n",
      " - 1s - loss: 0.5534 - acc: 0.7229 - val_loss: 0.5468 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/64\n",
      " - 1s - loss: 0.5394 - acc: 0.7404 - val_loss: 0.5271 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54196 to 0.52710, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 8/64\n",
      " - 1s - loss: 0.5312 - acc: 0.7459 - val_loss: 0.5203 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52710 to 0.52032, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 9/64\n",
      " - 1s - loss: 0.5267 - acc: 0.7488 - val_loss: 0.5088 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.52032 to 0.50878, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 10/64\n",
      " - 1s - loss: 0.5239 - acc: 0.7446 - val_loss: 0.5053 - val_acc: 0.7613\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50878 to 0.50532, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 11/64\n",
      " - 1s - loss: 0.5000 - acc: 0.7615 - val_loss: 0.5048 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50532 to 0.50483, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 12/64\n",
      " - 1s - loss: 0.5018 - acc: 0.7635 - val_loss: 0.5282 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/64\n",
      " - 1s - loss: 0.5066 - acc: 0.7556 - val_loss: 0.5465 - val_acc: 0.7218\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/64\n",
      " - 1s - loss: 0.5109 - acc: 0.7501 - val_loss: 0.5068 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/64\n",
      " - 1s - loss: 0.4961 - acc: 0.7626 - val_loss: 0.5366 - val_acc: 0.7239\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/64\n",
      " - 1s - loss: 0.4928 - acc: 0.7650 - val_loss: 0.4821 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50483 to 0.48210, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 17/64\n",
      " - 1s - loss: 0.4884 - acc: 0.7732 - val_loss: 0.4887 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/64\n",
      " - 1s - loss: 0.4763 - acc: 0.7800 - val_loss: 0.4874 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/64\n",
      " - 1s - loss: 0.4652 - acc: 0.7846 - val_loss: 0.4998 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/64\n",
      " - 1s - loss: 0.4543 - acc: 0.7877 - val_loss: 0.4842 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/64\n",
      " - 1s - loss: 0.4520 - acc: 0.7940 - val_loss: 0.4885 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/64\n",
      " - 1s - loss: 0.4438 - acc: 0.7956 - val_loss: 0.4585 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48210 to 0.45846, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 23/64\n",
      " - 1s - loss: 0.4558 - acc: 0.7811 - val_loss: 0.4967 - val_acc: 0.7679\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/64\n",
      " - 1s - loss: 0.4465 - acc: 0.7903 - val_loss: 0.4657 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/64\n",
      " - 1s - loss: 0.4370 - acc: 0.8022 - val_loss: 0.5279 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/64\n",
      " - 1s - loss: 0.4739 - acc: 0.7732 - val_loss: 0.4856 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/64\n",
      " - 1s - loss: 0.4554 - acc: 0.7866 - val_loss: 0.5051 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/64\n",
      " - 1s - loss: 0.4398 - acc: 0.7940 - val_loss: 0.4509 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.45846 to 0.45087, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 29/64\n",
      " - 1s - loss: 0.4297 - acc: 0.8043 - val_loss: 0.5658 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/64\n",
      " - 1s - loss: 0.4545 - acc: 0.7899 - val_loss: 0.4725 - val_acc: 0.7787\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/64\n",
      " - 1s - loss: 0.4220 - acc: 0.8039 - val_loss: 0.4520 - val_acc: 0.7976\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/64\n",
      " - 1s - loss: 0.4295 - acc: 0.7978 - val_loss: 0.5303 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/64\n",
      " - 1s - loss: 0.4393 - acc: 0.8026 - val_loss: 0.4778 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/64\n",
      " - 1s - loss: 0.4211 - acc: 0.8046 - val_loss: 0.4775 - val_acc: 0.7725\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/64\n",
      " - 1s - loss: 0.4423 - acc: 0.7940 - val_loss: 0.4546 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/64\n",
      " - 1s - loss: 0.4236 - acc: 0.8011 - val_loss: 0.4487 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.45087 to 0.44866, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 37/64\n",
      " - 1s - loss: 0.4209 - acc: 0.8063 - val_loss: 0.4769 - val_acc: 0.7782\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/64\n",
      " - 1s - loss: 0.4289 - acc: 0.7989 - val_loss: 0.4488 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/64\n",
      " - 1s - loss: 0.4237 - acc: 0.8041 - val_loss: 0.4551 - val_acc: 0.7971\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/64\n",
      " - 1s - loss: 0.4060 - acc: 0.8134 - val_loss: 0.4809 - val_acc: 0.7582\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/64\n",
      " - 1s - loss: 0.4491 - acc: 0.7868 - val_loss: 0.4495 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/64\n",
      " - 1s - loss: 0.4166 - acc: 0.8061 - val_loss: 0.4488 - val_acc: 0.7992\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/64\n",
      " - 1s - loss: 0.4189 - acc: 0.8074 - val_loss: 0.4674 - val_acc: 0.7818\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/64\n",
      " - 1s - loss: 0.4158 - acc: 0.8123 - val_loss: 0.4526 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/64\n",
      " - 1s - loss: 0.4088 - acc: 0.8098 - val_loss: 0.4520 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/64\n",
      " - 1s - loss: 0.3974 - acc: 0.8171 - val_loss: 0.4576 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/64\n",
      " - 1s - loss: 0.3942 - acc: 0.8171 - val_loss: 0.4470 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.44866 to 0.44696, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 48/64\n",
      " - 1s - loss: 0.3938 - acc: 0.8217 - val_loss: 0.4508 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/64\n",
      " - 1s - loss: 0.4106 - acc: 0.8164 - val_loss: 0.4600 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/64\n",
      " - 1s - loss: 0.4054 - acc: 0.8155 - val_loss: 0.4532 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/64\n",
      " - 1s - loss: 0.3960 - acc: 0.8162 - val_loss: 0.5351 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/64\n",
      " - 1s - loss: 0.3934 - acc: 0.8252 - val_loss: 0.4520 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/64\n",
      " - 1s - loss: 0.4003 - acc: 0.8208 - val_loss: 0.4768 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/64\n",
      " - 1s - loss: 0.4164 - acc: 0.8079 - val_loss: 0.4455 - val_acc: 0.7976\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.44696 to 0.44551, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 55/64\n",
      " - 1s - loss: 0.3897 - acc: 0.8224 - val_loss: 0.4521 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/64\n",
      " - 1s - loss: 0.3854 - acc: 0.8303 - val_loss: 0.4481 - val_acc: 0.7976\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/64\n",
      " - 1s - loss: 0.4243 - acc: 0.7989 - val_loss: 0.4430 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.44551 to 0.44299, saving model to Thinking_Feeling_DNN.weights.best.hdf5\n",
      "Epoch 58/64\n",
      " - 1s - loss: 0.3977 - acc: 0.8173 - val_loss: 0.4514 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/64\n",
      " - 1s - loss: 0.3933 - acc: 0.8254 - val_loss: 0.4686 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/64\n",
      " - 1s - loss: 0.3798 - acc: 0.8325 - val_loss: 0.4480 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/64\n",
      " - 1s - loss: 0.3785 - acc: 0.8305 - val_loss: 0.4451 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/64\n",
      " - 1s - loss: 0.3876 - acc: 0.8254 - val_loss: 0.4945 - val_acc: 0.7715\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/64\n",
      " - 1s - loss: 0.4131 - acc: 0.8131 - val_loss: 0.4483 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/64\n",
      " - 1s - loss: 0.4012 - acc: 0.8175 - val_loss: 0.5001 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(300, activation='relu', input_shape=(300,)))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.summary()\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, frame['t'], test_size=0.25, random_state=1, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='Thinking_Feeling_DNN.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "hist = model2.fit(X_train, y_train, batch_size = 64, epochs =64, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy:  0.8045182110472944\n",
      "AUC:  0.8836045900251284\n",
      "F1 Score:  0.8028846153846154\n"
     ]
    }
   ],
   "source": [
    "# load the model weights that had the best validation score\n",
    "model2.load_weights('Thinking_Feeling_DNN.weights.best.hdf5')\n",
    "\n",
    "# evaluate the model's accuracy\n",
    "score = model2.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "# print the accuracy\n",
    "print('\\nTest accuracy: ', score[1])\n",
    "\n",
    "y_pred = model2.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, (y_pred>0.445).astype(int), pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4554 samples, validate on 1952 samples\n",
      "Epoch 1/30\n",
      " - 1s - loss: 0.4680 - acc: 0.7778 - val_loss: 0.7068 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70676, saving model to Perceiving_Judging_DNN.weights.best.hdf5\n",
      "Epoch 2/30\n",
      " - 1s - loss: 0.4599 - acc: 0.7828 - val_loss: 0.7788 - val_acc: 0.6824\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/30\n",
      " - 1s - loss: 0.4580 - acc: 0.7828 - val_loss: 0.7080 - val_acc: 0.6952\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/30\n",
      " - 1s - loss: 0.4631 - acc: 0.7809 - val_loss: 0.7148 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/30\n",
      " - 1s - loss: 0.4684 - acc: 0.7736 - val_loss: 0.7264 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/30\n",
      " - 1s - loss: 0.4565 - acc: 0.7804 - val_loss: 0.7139 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/30\n",
      " - 1s - loss: 0.4822 - acc: 0.7611 - val_loss: 0.7253 - val_acc: 0.7065\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/30\n",
      " - 1s - loss: 0.4565 - acc: 0.7841 - val_loss: 0.7395 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/30\n",
      " - 1s - loss: 0.4728 - acc: 0.7631 - val_loss: 0.7031 - val_acc: 0.7013\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.70676 to 0.70311, saving model to Perceiving_Judging_DNN.weights.best.hdf5\n",
      "Epoch 10/30\n",
      " - 1s - loss: 0.4578 - acc: 0.7817 - val_loss: 0.6996 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70311 to 0.69964, saving model to Perceiving_Judging_DNN.weights.best.hdf5\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.4483 - acc: 0.7855 - val_loss: 0.7643 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.4513 - acc: 0.7885 - val_loss: 0.7502 - val_acc: 0.6885\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.4631 - acc: 0.7784 - val_loss: 0.7157 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.4572 - acc: 0.7857 - val_loss: 0.7492 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.4557 - acc: 0.7826 - val_loss: 0.7335 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.4566 - acc: 0.7855 - val_loss: 0.7381 - val_acc: 0.6783\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.4654 - acc: 0.7787 - val_loss: 0.7059 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.4435 - acc: 0.7916 - val_loss: 0.7633 - val_acc: 0.7059\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.4559 - acc: 0.7839 - val_loss: 0.7810 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.4599 - acc: 0.7758 - val_loss: 0.7530 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.4571 - acc: 0.7762 - val_loss: 0.7413 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.4463 - acc: 0.7885 - val_loss: 0.7817 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.4434 - acc: 0.7850 - val_loss: 0.7326 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.4561 - acc: 0.7795 - val_loss: 0.7553 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.4387 - acc: 0.7910 - val_loss: 0.7733 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/30\n",
      " - 1s - loss: 0.4625 - acc: 0.7723 - val_loss: 0.7907 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.4718 - acc: 0.7754 - val_loss: 0.7607 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.4458 - acc: 0.7910 - val_loss: 0.7676 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.4458 - acc: 0.7890 - val_loss: 0.8077 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.4590 - acc: 0.7837 - val_loss: 0.6960 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.69964 to 0.69601, saving model to Perceiving_Judging_DNN.weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, frame['p'], test_size=0.25, random_state=1, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='Perceiving_Judging_DNN.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "#hist = model.fit(X_train, y_train, batch_size = 64, epochs =30, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)\n",
    "hist = model2.fit(X_train, y_train, batch_size = 32, epochs =30, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy:  0.7040110648969946\n",
      "AUC:  0.7505027390610914\n",
      "F1 Score:  0.7830723092302565\n"
     ]
    }
   ],
   "source": [
    "# load the model weights that had the best validation score\n",
    "model2.load_weights('Perceiving_Judging_DNN.weights.best.hdf5')\n",
    "\n",
    "# evaluate the model's accuracy\n",
    "score = model2.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "# print the accuracy\n",
    "print('\\nTest accuracy: ', score[1])\n",
    "\n",
    "y_pred = model2.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, (y_pred>0.43).astype(int), pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_personality(string):\n",
    "    weights = ['Extrovert_Introvert_DNN.weights.best.hdf5', 'Intuitive_Sensing_DNN.weights.best.hdf5']\n",
    "    weights2 = ['Thinking_Feeling_DNN.weights.best.hdf5', 'Perceiving_Judging_DNN.weights.best.hdf5']\n",
    "\n",
    "    guess = \"\"\n",
    "\n",
    "    for w in weights:\n",
    "        model.load_weights(w)\n",
    "        doc = nlp(string)\n",
    "        tester = doc.vector.reshape(1, -1)\n",
    "        prediction = model.predict(tester)\n",
    "        if w == weights[0]:\n",
    "            print(\"Introverted = 0, Extroverted = 1: \", prediction[0][0])\n",
    "            if prediction[0][0] < 0.346:\n",
    "                guess += \"I\"\n",
    "            else:\n",
    "                guess += \"E\"\n",
    "        if w == weights[1]:\n",
    "            print(\"Sensing = 0, INtuitive = 1: \", prediction[0][0])\n",
    "            if prediction[0][0] < 0.5:\n",
    "                guess += \"S\"\n",
    "            else:\n",
    "                guess += \"N\"\n",
    "    for w in weights2:\n",
    "        model2.load_weights(w)\n",
    "        doc = nlp(string)\n",
    "        tester = doc.vector.reshape(1, -1)\n",
    "        prediction = model.predict(tester)\n",
    "        if w == weights2[0]:\n",
    "            print(\"Feeling = 0, Thinking = 1: \", prediction[0][0])\n",
    "            if prediction[0][0] < 0.445:\n",
    "                guess += \"F\"\n",
    "            else:\n",
    "                guess += \"T\"\n",
    "        if w == weights2[1]:\n",
    "            print(\"Judging = 0, Perceiving = 1: \", prediction[0][0])\n",
    "            if prediction[0][0] < 0.43:\n",
    "                guess += \"J\"\n",
    "            else:\n",
    "                guess += \"P\"\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introverted = 0, Extroverted = 1:  0.9480639\n",
      "Sensing = 0, INtuitive = 1:  0.7915955\n",
      "Feeling = 0, Thinking = 1:  0.7915955\n",
      "Judging = 0, Perceiving = 1:  0.7915955\n",
      "ENTP\n"
     ]
    }
   ],
   "source": [
    "string = \"Enter anything you'd like here and I'll guess your personality. I'm and ENTP, so let's see how it works\"\n",
    "\n",
    "guess = guess_personality(string)\n",
    "print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "The final modelâ€™s qualities â€” such as parameters â€” are evaluated in detail. Some type of analysis is used to validate the robustness of the modelâ€™s solution.\n",
    "\n",
    "### Justification\n",
    "\n",
    "Although the final model performed much better than the benchmark, I would not say this solution adequately solves the problem of developing a tool to help understand people's personality through their language.  Simply getting near 80% accuracy of understanding a generalization of people is not enough. Much work is to be done before we can begin to use our computers as tools for understanding human personality and behavior in a meaningful way.\n",
    "\n",
    "The final results with the comparieson to the benchmarks are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusion\n",
    "\n",
    "### Free Form Visualization\n",
    "\n",
    "### Extroversion vs. Introversion\n",
    "#### LogisticRegression\n",
    "Accuracy:  0.7846934071000461\n",
    "AUC:  0.51472747716303\n",
    "F1 Score:  0.06786427145708582\n",
    "#### RandomForestClassifier\n",
    "Accuracy:  0.7699400645458737\n",
    "AUC:  0.5228257509274403\n",
    "F1 Score:  0.1381692573402418\n",
    "#### SVC\n",
    "Accuracy:  0.7819271553711388\n",
    "AUC:  0.5\n",
    "#### MLPClassifier\n",
    "Accruacy:  0.8063623789764869\n",
    "AUC:  0.6383506521999283\n",
    "F1 Score:  0.4339622641509434\n",
    "#### Keras Deep Learning Model\n",
    "Test accuracy:  0.8137390501436521\n",
    "AUC:  0.8061750849782338\n",
    "F1 Score:  0.5853211009174313\n",
    "\n",
    "\n",
    "### INtuitive vs. Sensing\n",
    "#### LogisticRegression\n",
    "Accuracy:  0.8575380359612724\n",
    "AUC:  0.49946294307196565\n",
    "F1 Score:  0.9233060312732689\n",
    "#### RandomForestClassifier\n",
    "Accuracy:  0.8409405255878285\n",
    "AUC:  0.5210790470825739\n",
    "F1 Score:  0.9125918419052446\n",
    "#### SVC\n",
    "Accuracy:  0.8584601198709082\n",
    "AUC:  0.5\n",
    "F1 Score:  0.923840238154304\n",
    "#### MLPClassifier\n",
    "Accruacy:  0.8630705394190872\n",
    "AUC:  0.5380488214486892\n",
    "F1 Score:  0.925545249435949\n",
    "#### Keras Deep Learning Model\n",
    "Test accuracy:  0.8810511753547021\n",
    "AUC:  0.7696554209099011\n",
    "F1 Score:  0.9354677338669335\n",
    "\n",
    "\n",
    "### Thinking vs. Feeling\n",
    "#### LogisticRegression\n",
    "Accuracy:  0.7584140156754264\n",
    "AUC:  0.7539748631351686\n",
    "F1 Score:  0.7265135699373694\n",
    "#### RandomForestClassifier\n",
    "Accuracy:  0.6685108344859382\n",
    "AUC:  0.6569473359549696\n",
    "F1 Score:  0.5907797381900968\n",
    "#### SVC\n",
    "Accuracy:  0.7736284001844168\n",
    "AUC:  0.7708844166859433\n",
    "F1 Score:  0.7488491048593351\n",
    "#### MLPClassifier\n",
    "Accruacy:  0.7897648686030428\n",
    "AUC:  0.7879944482997918\n",
    "F1 Score:  0.7692307692307693\n",
    "#### Keras Deep Learning Model\n",
    "Test accuracy:  0.8045182110472944\n",
    "AUC:  0.8836045900251284\n",
    "F1 Score:  0.8028846153846154\n",
    "\n",
    "### Perceiving vs. Jud\n",
    "#### LogisticRegression\n",
    "Accuracy:  0.6454587367450438\n",
    "AUC:  0.5728658574786054\n",
    "F1 Score:  0.7559504919073311\n",
    "#### RandomForestClassifier\n",
    "Accuracy:  0.5859843245735362\n",
    "AUC:  0.5685350128072716\n",
    "F1 Score:  0.6564651874521805\n",
    "#### SVC\n",
    "Accuracy:  0.6256339326878746\n",
    "AUC:  0.5206361750380817\n",
    "F1 Score:  0.763953488372093\n",
    "#### MLPClassifier\n",
    "Accruacy:  0.6809589672660212\n",
    "AUC:  0.647889977392615\n",
    "F1 Score:  0.7532097004279601\n",
    "#### Keras Deep Learning Model\n",
    "Test accuracy:  0.7040110648969946\n",
    "AUC:  0.7505027390610914\n",
    "F1 Score:  0.7830723092302565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/courses/cs224n/2015/reports/6.pdf\n",
    "#### Understanding Personality through Social Media\n",
    "#### AUC\n",
    "    E/I - \n",
    "        Theirs: 0.691\n",
    "        Mine: 0.806\n",
    "    N/S - \n",
    "        Theirs: 0.653\n",
    "        Mine: 0.7696\n",
    "    T/F - \n",
    "        Theirs: 0.68\n",
    "        Mine: 0.8836\n",
    "    P/J -\n",
    "        Theirs: 0.61\n",
    "        Mine: 0.75\n",
    "        \n",
    "    Average -\n",
    "        Theirs: 0.661\n",
    "        Mine: 0.803\n",
    "\n",
    "https://www.kaggle.com/depture/multiclass-and-multi-output-classification/notebook\n",
    "#### Best Kaggle Project I could make sense of\n",
    "\n",
    "#### F1_Score\n",
    "    E/I - \n",
    "        Theirs: 0.59\n",
    "        Mine: 0.585\n",
    "    N/S - \n",
    "        Theirs: 0.44\n",
    "        Mine: 0.935\n",
    "    T/F - \n",
    "        Theirs: 0.8\n",
    "        Mine: 0.80288\n",
    "    P/J -\n",
    "        Theirs: 0.83\n",
    "        Mine: 0.783\n",
    "        \n",
    "    Average -\n",
    "        Theirs: 0.665\n",
    "        Mine: 0.776\n",
    "        \n",
    "#### My Accuracy\n",
    "    E/I - 0.8137\n",
    "    N/S - 0.881\n",
    "    T/F - 0.804\n",
    "    P/J - 0.704\n",
    "    \n",
    "    Average - 0.800675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "One particular thing I found difficult was preprocessing the data for the Keras/ Tensorflow Neural Network.  For the simple classifiers I had to input a 1D stream of data, whereas I needed 3 dimensions for the LSTM NN. At first I was getting very confused, going back and forth between different preprocessing methods getting one to fit and then not the other. I did not realize that I needed to make two separate datasets in order to make this project work.  However, what I found interesting was that the SpaCy word vectors take into account dependencies and time series when creating word vectors. Since this is information is encapsulated, then the LSTM can focus on more broad patterns than one understanding and predicting the likelihood of seeing the phrase.  Finding the right mix of abstraction of language without losing information in the averaged word vectors will be integral in understanding the patterns of the human psyche through language.\n",
    "\n",
    "Another interesting thing I had trouble with was one-hot encoding the personality data. Since there are 16 different personality types, one method would be to make each prediction class equivalent to one personality type resulting in an array of length 16. Another way would be to predict whether or not each person was Introverted, Extroverted, iNtuitive, Sensing, Thinking, Feeling, Perceiving, or Judging, resulting in an array of length 8.  Or even take it one step further, since one cannot be both Introverted and Extroverted according to the MBTI, we could assign Extroverted a value of 1 and Introverted a value of 0.  This would result in an array of length 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "One aspect of the implementation that could have been improved making 2D or 3D tensors and cleaning the data in a way that didn't require losing people with less than 50 posts and any post after the 50th one, losing information from links, or result in a skewed distribution of classes in the dataset. I ran into trouble just cleaning the data, and never was able to get a 2D or greater tensor to fit into keras. This way, the most information would be preserved by the corpus of the users and the model would result in a more transferrable prediction to the general population, rather than specifically Kaggle members.  This would increase the dimensionality of the data further and allow for a convolutional neural network, however, this takes more than 2 days to process on my quad-core processor and requires padding or otherwise losing information in some posts as some people use more words than others.\n",
    "\n",
    "Other improvements would be to use much better data, this is clearly skewed toward Introverts and Intuitive Functioning personalities.  Getting more information from links, removing the types from posts, as well as adding other features like post count, number of words per post, characters per word, weighing certain words more heavily, among many many others.  I also tried breaking the problem into smaller sections, learning on introverts to create weights that would better predict the remaining features given the person is introverted. This way, I could create hidden markov models with hidden variables that are extracted from individual filters to create a new model that would predict more accurately.\n",
    "\n",
    "Below are some way's I attempted this among many other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Anything below is just code from me messing around too much, learning, or otherwise getting off task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_list = []\n",
    "df_label_list = []\n",
    "df_vector_norm_list = []\n",
    "df_post_list = []\n",
    "df_index_list = []\n",
    "e_list = []\n",
    "n_list = []\n",
    "t_list = [] \n",
    "p_list = []\n",
    "word_vectors = []\n",
    "vector_norms = []\n",
    "\n",
    "\n",
    "for i in range(len(data.type)):\n",
    "    # go through each person in the dataframe, with a fresh list of vectors for \n",
    "    #persons_comments = \"\"\n",
    "    #list_of_vectors = []\n",
    "    #list_of_vector_norms = []\n",
    "    persons_comments = \"\"\n",
    "    for post in data.posts[i]:\n",
    "        \n",
    "    label = [data.iloc[i]['e'], data.iloc[i]['n'], data.iloc[i]['t'], data.iloc[i]['p']]\n",
    "\n",
    "    for post in data.posts[i]:\n",
    "        #persons_comments += post + \". \"\n",
    "        persons_comments += post + \" \"\n",
    "\n",
    "        doc = nlp(post)\n",
    "        df_vector_list.append(doc.vector)\n",
    "        df_vector_norm_list.append(doc.vector_norm)\n",
    "        df_post_list.append(post)\n",
    "        df_index_list.append(i)\n",
    "        df_label_list.append(label)\n",
    "        e_list.append(label[0])\n",
    "        n_list.append(label[1])\n",
    "        t_list.append(label[2]) \n",
    "        p_list.append(label[3])\n",
    "    # after each post has been gone through but before the moving on to the next person\n",
    "        # I want a list of vector norms to append to df vector norm list\n",
    "        # I want a vector describing the whole corpus\n",
    "    #vector.append(doc.vector)\n",
    "    #print(vectors.dtype)\n",
    "    #data['word_vectors'][i] = vectors\n",
    "    doc = nlp(persons_comments)\n",
    "    word_vectors.append(doc.vector)\n",
    "    vector_norms.append(doc.vector_norm)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675.)if i%500 == 0:\n",
    "        print(i)\n",
    "\n",
    "post_data = { 'user_id' :  df_index_list,\n",
    "              'label'   :  df_label_list,\n",
    "        'word_vector'   :  df_vector_list,\n",
    "        'vector_norm'   :  df_vector_norm_list,\n",
    "             'post'     :  df_post_list,\n",
    "             'links'    :  df_links_list\n",
    "             'e'        :  e_list,\n",
    "             'n'        :  n_list,\n",
    "             't'        :  t_list,\n",
    "             'p'        :  p_list\n",
    "            }\n",
    "post_df = pd.DataFrame.from_dict(post_data)\n",
    "print(post_df.head())\n",
    "\n",
    "for person in data.type:\n",
    "    if check_E(person):\n",
    "        if check_N(person):\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(1) #[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ENTP\n",
    "                else:\n",
    "                    label_list.append(2) #[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(3) #[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENFP\n",
    "                else:\n",
    "                    label_list.append(4) #[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENFJ\n",
    "                \n",
    "        else:\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(5) #[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESTP\n",
    "                else:\n",
    "                    label_list.append(6) #[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(7) #[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESFP\n",
    "                else:\n",
    "                    label_list.append(8) #[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] # ESFJ\n",
    "    else:\n",
    "        if check_N(person):\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(9) #[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # INTP\n",
    "                else:\n",
    "                    label_list.append(10) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] # INTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(11) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] # INFP\n",
    "                else:\n",
    "                    label_list.append(12) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] # INFJ\n",
    "                \n",
    "        else:\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(13) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] # ISTP\n",
    "                else:\n",
    "                    label_list.append(14) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] # ISTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(15) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] # ISFP\n",
    "                else:\n",
    "                    label_list.append(16) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] # ISFJ\n",
    "    \n",
    "print(label_list)\n",
    "\n",
    "\n",
    "high_dim_vectors = []\n",
    "\n",
    "for i in range(10):\n",
    "    persons_hd_vectors = []\n",
    "    persons_comments = \"\"\n",
    "    #    print(data.posts[i])\n",
    "    #for post in data.posts[i]:\n",
    "    #    print(post)\n",
    "    #    persons_comments += post + \" \"\n",
    "    print(data.posts[i][:100])\n",
    "    doc = nlp(data.posts[i][:100])\n",
    "    persons_hd_vectors.append(doc.tensor)\n",
    "    print(persons_hd_vectors[].shape)\n",
    "    \n",
    "# Go through each post, convert it to a doc.vector and it's associated type\n",
    "post_list = []\n",
    "type_by_post_list = []\n",
    "vector = np.empty((7684, 50, 300))\n",
    "vector_list = []\n",
    "i = 0\n",
    "for posts in frame.posts:\n",
    "    t = type_dict[i]\n",
    "    print(t)\n",
    "    p = np.empty((50,300))\n",
    "    for j in range(50):\n",
    "        string = posts[j].tostring()\n",
    "        strn = string.decode('UTF-16')\n",
    "        print(strn)\n",
    "        doc = nlp(strn)\n",
    "        post_list.append(doc.vector)\n",
    "        type_by_post_list.append(t)\n",
    "        p[j] = doc.vector\n",
    "    vector[i] = p\n",
    "    vector_list.append(p)\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "    print(i)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    \n",
    "print(len(post_list))\n",
    "print(len(type_by_post_list)) \n",
    "#print(len(vector))\n",
    "print(len(vector_list))\n",
    "#print(len(type_dict))\n",
    "\n",
    "\n",
    "\n",
    "posts_dict = {\n",
    "    \"label\" : type_by_post_list\n",
    "    \"vector\" : post_list\n",
    "}\n",
    "\n",
    "posts_df = pd.DataFrame.fromdict(posts_dict)\n",
    "print(posts_df.head())\n",
    "\n",
    "import re\n",
    "\n",
    "# Split the data at |||\n",
    "vector = np.empty((8675, 30, 300))\n",
    "simple_vector = np.empty(())\n",
    "drop_list = []\n",
    "\n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "for i in range(len(data.posts)):  \n",
    "    p = np.empty((30,300))\n",
    "    posts = data.posts[i]\n",
    "    #print(posts)\n",
    "    #data.posts[i] = posts.split('|||')\n",
    "    posts = pattern.sub('link', posts)\n",
    "    newnew = posts.split('|||')\n",
    "    if len(newnew) >= 30:\n",
    "        for j in range(30):\n",
    "            doc = nlp(newnew[j])\n",
    "            p[j] = doc.vector\n",
    "        vector[i] = p\n",
    "    else:\n",
    "        #print(i, len(newnew))\n",
    "        drop_list.append(i)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675., \"% done\")\n",
    "        \n",
    "print(data.head())\n",
    "\n",
    "word_vectors = []\n",
    "vector_norms = []\n",
    "data['e'] = 0\n",
    "data['n'] = 0\n",
    "data['t'] = 0\n",
    "data['p'] = 0\n",
    "    \n",
    "for i in range(1): #len(data.type)\n",
    "    persons_comments = \"\"\n",
    "    if data.type[i][0] == 'E':\n",
    "        data['e'][i] = 1\n",
    "    if data.type[i][1] == 'N':\n",
    "        data['n'][i] = 1\n",
    "    if data.type[i][2] == 'T':\n",
    "        data['t'][i] = 1\n",
    "    if data.type[i][3] == 'P':\n",
    "        data['p'][i] = 1\n",
    "    #for post in data.posts[i]:\n",
    "    #    persons_comments += post + \" \"\n",
    "    #doc = nlp(persons_comments)\n",
    "    doc = nlp(data.posts[i])\n",
    "    word_vectors.append(doc.vector)\n",
    "    vector_norms.append(doc.vector_norm)\n",
    "    tester = doc.vector.reshape(1, -1)\n",
    "    print(tester.shape)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675.)\n",
    "\n",
    "print(data.tail())\n",
    "\n",
    "\n",
    "average_vector = []\n",
    "i=0\n",
    "\n",
    "for posts in keep_list:\n",
    "    post = \"\"\n",
    "    for p in posts:\n",
    "        post += p + \" \"\n",
    "    doc = nlp(post)\n",
    "    average_vector.append(doc.vector)\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2, activation ='relu', input_shape=(50, 1, 300)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(16, activation='softmax')) #softmax #tanh\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "posts_sentiment_list = []\n",
    "\n",
    "i=0\n",
    "for posts in frame.posts[:5]:\n",
    "    p = []\n",
    "    for i in range(len(posts)): \n",
    "        string = posts[i].tostring()\n",
    "        s = string.decode('UTF-32')\n",
    "        post_doc = nlp(s)\n",
    "        p.append(post_doc.sentiment)\n",
    "    posts_sentiment_list.append(p)\n",
    "    i+=1\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "print(posts_sentiment_list)\n",
    "\n",
    "def type_to_numbers(type_list):\n",
    "    \n",
    "    hot_label_list = []\n",
    "    for i in type_list:\n",
    "        if i == 'ENTP':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 1, 0, 1, 0]))\n",
    "        if i == 'INTP':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 1, 0, 1, 0]))\n",
    "        if i == 'ESTP':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 1, 0, 1, 0]))\n",
    "        if i == 'ISTP':\n",
    "            hot_label_list.append(np.array([ 0, 1, 0, 1, 1, 0, 1, 0]))\n",
    "        if i == 'ENFP':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 0, 1, 1, 0]))\n",
    "        if i == 'INFP':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 0, 1, 1, 0]))\n",
    "        if i == 'ESFP':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 0, 1, 1, 0]))\n",
    "        if i == 'ISFP':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 0, 1, 1, 0]))\n",
    "        if i == 'ENTJ':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 1, 0, 0, 1]))\n",
    "        if i == 'INTJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ESTJ':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ISTJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ENFJ':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 0, 1, 0, 1]))\n",
    "        if i == 'INFJ':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 0, 1, 0, 1]))\n",
    "        if i == 'ESFJ':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 0, 1, 0, 1]))\n",
    "        if i == 'ISFJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 0, 1, 0, 1]))\n",
    "        \n",
    "    return hot_label_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "frame['hot_label'] = type_to_numbers(type_list)\n",
    "    \n",
    "print(len(type_list))\n",
    "\n",
    "print(len(posts_vector_2d_list))\n",
    "print(len(posts_vector_1d_list))\n",
    "posts_vector_2d_list = word_tensors_df.values\n",
    "print(len(posts_vector_2d_list))\n",
    "\n",
    "word_vectors_df = pd.DataFrame({\n",
    "    'posts' : posts_vector_1d_list,\n",
    "    'label' : label_list\n",
    "})\n",
    "\n",
    "print(word_vectors_df.head())\n",
    "\n",
    "word_tensors_df = pd.DataFrame({\n",
    "    'posts' : posts_vector_2d_list,\n",
    "    'label' : label_list\n",
    "})\n",
    "\n",
    "print(word_tensors_df.head())\n",
    "\n",
    "word_vectors_df.to_csv(\"word_vectors.csv\")\n",
    "word_tensors_df.to_csv(\"word_tenors.csv\")\n",
    "\n",
    "wvdf = pd.read_csv(\"word_vectors.csv\")\n",
    "wtdf = pd.read_csv(\"word_tenors.csv\")\n",
    "print(wvdf.head())\n",
    "\n",
    "\n",
    "# Remove any people with posts that have less than 50 posts in them\n",
    "# Split the posts and save them as an array \n",
    "#drop_list = []\n",
    "keep_list = []\n",
    "type_list = []\n",
    "\n",
    "df = data\n",
    "df['num_posts'] = 0\n",
    "\n",
    "for i in range(len(df.type)): \n",
    "    string = df['posts'][i]\n",
    "    temp = np.array(string.split('|||'))\n",
    "    #if len(temp) < 50:\n",
    "    #    drop_list.append(i)\n",
    "    if len(temp) >= 50:\n",
    "        df['num_posts'][i] = 50\n",
    "        temp = temp[:50]\n",
    "        keep_list.append(temp)\n",
    "        type_list.append(df['type'][i])\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "uniform_df = df[df[\"num_posts\"] == 50]\n",
    "uniform_df.reset_index(drop=False)\n",
    "print(uniform_df.tail())\n",
    "\n",
    "frame = pd.DataFrame({\n",
    "    'posts' : keep_list,\n",
    "    'type'  : type_list\n",
    "})\n",
    "\n",
    "# https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "         self.val_f1s = []\n",
    "         self.val_recalls = []\n",
    "         self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "         val_predict = (np.asarray(self.model.predict(X_val))).round()\n",
    "         val_targ = y_val\n",
    "         _val_f1 = f1_score(val_targ, val_predict)\n",
    "         _val_recall = recall_score(val_targ, val_predict)\n",
    "         _val_precision = precision_score(val_targ, val_predict)\n",
    "         self.val_f1s.append(_val_f1)\n",
    "         self.val_recalls.append(_val_recall)\n",
    "         self.val_precisions.append(_val_precision)\n",
    "         print(\"â€” val_f1: %f â€” val_precision: %f â€” val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "         return\n",
    " \n",
    "cb = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'e'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-13461c1e30a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEN_word_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mextroverts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mintroverts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'e'"
     ]
    }
   ],
   "source": [
    "EN_word_vector = []\n",
    "\n",
    "extroverts = data.loc[data['e'] == 1]\n",
    "introverts = data.loc[data['e'] == 0]\n",
    "\n",
    "intuitive_introverts = introverts.loc[data['n'] == 1]\n",
    "intuitive_extroverts = extroverts.loc[data['n'] == 1]\n",
    "\n",
    "for i in intuitive_extroverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in intuitive_extroverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    EN_word_vector.append(doc.vector)\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(EN_word_vector, intuitive_extroverts['t'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "nn_clf = MLPClassifier()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "    \n",
    "    \n",
    "IN_word_vector = []\n",
    "\n",
    "for i in intuitive_introverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in intuitive_introverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    IN_word_vector.append(doc.vector)\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(IN_word_vector, intuitive_introverts['t'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "nn_clf = MLPClassifier()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    y_pred = clf.predict(E_word_vector)\n",
    "    print(clf.__class__.__name__, accuracy_score(extroverts['t'], y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(extroverts['t'], y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "    \n",
    "    \n",
    "E_word_vector = []\n",
    "for i in extroverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in extroverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    E_word_vector.append(doc.vector)\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "        \n",
    "y_pred = nn_clf.predict(E_word_vector)\n",
    "print(clf.__class__.__name__, accuracy_score(extroverts['p'], y_pred))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(extroverts['p'], y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)\n",
    "\n",
    "#X_test_train, X_test_t, y_test_train, y_test_t = train_test_split(X_test, y_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test_train)    \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test_train, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_train, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "\n",
    "nn_clf_2 = MLPClassifier()\n",
    "#train_np = np.array(X_test_train)\n",
    "#print(len(train_np))\n",
    "#guess = y_pred.tolist()\n",
    "\n",
    "print(X_test_train[0])\n",
    "\n",
    "array = y_pred.reshape(-1, 1)\n",
    "df = pd.DataFrame ({\n",
    "    'vector': np.array(X_test_train,dtype='float32'),\n",
    "    'pred'  : array\n",
    "})\n",
    "\n",
    "#print(X_test_train[:2])\n",
    "#trainer = zip(X_test_train, guess)\n",
    "#xtestrain = np.ndarray(list(trainer))\n",
    "#print(xtestrain)\n",
    "nn_clf_2.fit(array, y_test_train)\n",
    "y_pred_2 = nn_clf_2.predict(df)\n",
    "print(clf.__class__.__name__, accuracy_score(y_test_t, y_pred_2))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_train, y_pred_2, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
