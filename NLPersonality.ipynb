{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone Project\n",
    "Danny Clifford\n",
    "December 31st, 2050\n",
    "\n",
    "\n",
    "# I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "With much better memory than people and the amount of personal information we share with computers, it's amazing they don't appear to understand our personalities better.  With the exceptions of saving searches and some companies using AI and machine learning to predict profitability for advertisements, there is little effort to understand personal tendencies to cater content for (and not just to) individuals. Even those multibillion dollar corporations aim to maximize advertisment profit, not to understand the fundamental features that make each of us unique.  \n",
    "\n",
    "Understanding what connects us all, what makes each of us unique, what are our strengths and how can we harness everyone's strengths to build better lives for individuals and humanity, seem's far from the focus of their efforts.  For the first time in history, we are able to economically collect and process enough information to understand the patterns of human nature.  There have been good attempts in recent history of distinguishing the features of human processing that make certain people different than others, most notably Carl Jung in his book Psychological Types and the subsequent adaptation of his theories to the Myers Briggs Personality Index.  \n",
    "\n",
    "Carl Jung notes that it is difficult for a person who experiences their own bias to accurately judge others, joking that one person creating a system would be like creating a Universal Church with one member.  Luckily, since his time wonderful scientists such as Alan Turing, John von Neumann, J.C.R. Licklider, Miller, Moore, Noyce, and countless other have made it incredibly easy to collect, share, and calculate data from around the world almost instantly, not to mention make impressive improvements on models of understanding how agents behave.  With the addition of breakthroughs in behavioral psychology by greats like Kahnemann, Tversky, and Thaler, we are quickly building the ability to study the patterns of reason and thought in humans as differentiated by the rational agents which traditional economic theory implies.\n",
    "\n",
    "Putting these pieces together, as we communicate with computers and people, we are creating valuable information and patterns that, if only captured and studied, would give great insight into how our individual and collective minds work.  This project is about helping computers understand how the patterns in our language reflect our inner personality and in turn how we recieve, process, and communicate information.  Ultimately, computers can be our tools to help us learn our unique patterns and to help us change, supplement, or leverage how we do things to help us achieve our goals.\n",
    "\n",
    "### Problem Statement\n",
    "    \n",
    "The problem that I am setting out to solve is how to understand someone's personality based on their use of language.  If we can accurately predict one of the most fundamental aspects of a person’s behavior and uniqueness based off the language they use, the ability to communicate information to that person will be drastically improved.  The internet is designed based on information that is already programmed into the web page itself; for example, administrators see a website much differently as a new customer or even a logged in user and are determined prior to visiting the webpage. This poses a difficult design problem for web designers to incorporate designs that maximize the profit or usefulness to their intended audience rather than communicating information or value to a person on an individual basis.  If we can predict learning style or how an individual will react to their environment, then we can better customize the learning experience to their preference. \n",
    "    \n",
    "Quantifying personality has been done for us with the Myers Brigg Personality Index 4 letter code. These will be further broken down into their 4 features of a single letter with only 2 options, making it a binary choice and easy to encode the data.  In addition, it will also allow us to train the weights of determining individual features of personality in a more focused way. NLP allows algorithms to extract meaning from text whether from word count, frequency, and even parts of speech in a quantifiable and measurable way. These matrices of language data will be learned by a Neural Network and these patterns during training will be used to predict the personality features of the test group.  \n",
    "\n",
    "1. Download the data from Kaggle\n",
    "2. Let SpaCy web-medium run through the posts to make word vectors\n",
    "3. Shuffle-Split the data into testing validation and training data.\n",
    "4. Run benchmarks training and testing with Logistic Regression, Random Forest, and MLP Classifiers\n",
    "5. Run training and validation through the CNN\n",
    "5. Test accuracy of the model on the test set with AUC\n",
    "    \n",
    "    \n",
    "### Metrics\n",
    "\n",
    "Training a neural network on language use and their corresponding personality feature labels allows us to measure the AUC.  Area under the ROC curve is used to ensure the proper binary classification when it comes to specificity and sensitivity. This will help better quantify individual differences in each of the 4 personality features. Wang uses AUC in order to quantify and measure accuracy of a model.  Since the distribution of personalities within the dataset is skewed in both our datasets, this will be a good evaluation metric to use.  He also measured accuracy by comparing different models based on the features mentioned above based on AUC, not only breaking them down into dichotomous features (Sensing and Intuitive, Extrovert and Introvert), but also by focusing on features of the language.  His best individual feature was average word vectors with an AUC of 0.651, which represents this model’s ability to predict all 4 of the dichotomous personality traits.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "The dataset is taken from Kaggle and contains 8,600 users with 50 recent comments on the Kaggle website each and their corresponding personality type.  This was user generated data from the Kaggle website and offers the most labeled personality data connected to their text data (comments) of what I could find online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the excel document with the results\n",
    "# save it as a panda's dataframe and call it data\n",
    "data = pd.read_csv('raw/mbti_1.csv')\n",
    "\n",
    "# print out a summary of the first 5 people to make sure it worked\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacu\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first 5 entries of the Kaggle dataset. It comes with a 4 letter code called 'type' which is the person's peronality archetype according to their test results from the Myers Briggs Personality Index. Under 'posts' is a string of their most recent 50 posts on Kaggle.com separated by |||. \n",
    "\n",
    "Below is a look at the first person's entire corpus of text that we can learn from in raw form. In Data Preprocessing we will remove the links and ||| along with creating a bag of words that a the person uses that we can compare to other people and personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words is a way to see how likely people are to use a word or phrase.  In Natural Language Processing (NLP), the likelihood of a word being used can be determined by how often people use the word. The likelihood of a person using a word is therefore in part determined by the similarity to the people who use the word multiplied by the likelihood of the word being said by anyone. If a person is extroverted and extroverts say a word 1% of the time and introverts 0.1% of the time; if that word is said then it is 10x more likely that it came from an extrovert.\n",
    "\n",
    "Using spaCy, \n",
    "\n",
    "Explain POS tagging\n",
    "\n",
    "Explain Weighted Average Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The bar graph below shows the distribution of the personality types in the Kaggle database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = data['type'].value_counts()\n",
    "k = data['type'].value_counts().keys()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(d.index, d.values, alpha=0.7)\n",
    "plt.ylabel('Instances', fontsize=12)\n",
    "plt.xlabel('Types', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly Introverted and/ or iNtuitive people dominate the Kaggle forums, or at least the ones participating in the creation of the database.  This will make it quite difficult to learn about those who are extroverted and sensing types (ESxx).  The model would likel minimize error by simply never predicting ESxx labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution = {}\n",
    "#actual_series = {}\n",
    "\n",
    "l = float(len(data))\n",
    "actual = [0.044, 0.015, 0.033, 0.021, 0.032, 0.081, 0.054, 0.088, 0.018, 0.116, 0.025, 0.138, 0.043, 0.085, 0.123, 0.087]\n",
    "kaggle = []\n",
    "\n",
    "for i in range(len(d)):\n",
    "    kaggle.append(float(d[i]) / l)\n",
    "\n",
    "keys = d.index.get_values()\n",
    "values = d.get_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import style \n",
    "\n",
    "fig, ax = plt.subplots(1) \n",
    "fig.autofmt_xdate()\n",
    "index = np.arange(16)\n",
    "bar_width = 7\n",
    "opacity = 0.7\n",
    "\n",
    "kaggle_bar = plt.bar(index*20, kaggle, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 label='Kaggle')\n",
    "actual_bar = plt.bar(index*20 + bar_width, actual, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 label='Actual')\n",
    "\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Percentage of Population')\n",
    "plt.title('Comparing Personality Distribution of Kaggle Dataset to the World')\n",
    "plt.xticks(index*20 + bar_width, keys)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above compares the percentage of people who belong to each type on the Kaggle database and Myers-Briggs world estimates.  If the data weren't already skewed enough, the people's personalities of this dataset are quite different to the general population.  When it comes to creating a system for understanding personality, the data used to train the model would ideally represent the population of personalities it is trying to understand.\n",
    "\n",
    "Insert Facets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "Preprocessing the data will be the largest part of the technique for predicting the personality of a person based on their text.  There are separating characters (|||) that need to be removed, links that will not show up in language and instead will be converted simply into the word 'link'.  \n",
    "\n",
    "As mentioned in the Data Exploration section above, using the bag of words technique to create word vectors of each personality type's language. Using spaCy's medium web language library to create relative frequencies of common language used online, the result will be a model of language use specific to each personality feature.  In addition, spaCy can find the similarities of new sentences to each corresponding personality and even between personalities.  \n",
    "\n",
    "The classifier is a Convolutional Neural Network, that will run over the language and assign the probability for each class based on their word vector representations and similarities to each personality type.\n",
    "\n",
    "Since the data is quite skewed not only from the population but especially not distributed evenly throughout the personality types, creating a model that will most accurately predict the personality type will likely result in simply guessing the most likely personality features, in this case Introverted and Intuitive (IN).  Tuning the model based on the AUC and series of 4 binary classifications (which coincidentally also makes more sense in the study of personality) allows to create the most potentially useful prediction model and avoid overfitting.  \n",
    "\n",
    "Calculating AUC not only tries to get the most predictions right, but tries to ensure that false positives and false negatives don't crop up.  In other words, since the accuracy metric would tend to create a prediction where everyone is IN__, the AUC metric will catch that the model systematically falsely assigns Extroverts the Introvert label and will look for patterns to help correct it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "The Benchmark I will be using to predict personality based on the data is a Logistic Regression Algoirthm with 10 fold cross validation, as I am trying to make a similar model to Yilun Wang's in his Understanding Personality through Social Media.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "(Just upload it from below, this takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any people with posts that have less than 50 posts in them\n",
    "# Split the posts and save them as an array \n",
    "drop_list = []\n",
    "keep_list = []\n",
    "type_list = []\n",
    "#keep_dict = {}\n",
    "#type_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "df = data\n",
    "df['num_posts'] = 0\n",
    "\n",
    "for i in range(len(df.type)): \n",
    "    string = df['posts'][i]\n",
    "    temp = np.array(string.split('|||'))\n",
    "    if len(temp) < 50:\n",
    "        drop_list.append(i)\n",
    "    if len(temp) >= 50:\n",
    "        df['num_posts'][i] = 50\n",
    "        temp = temp[:50]\n",
    "        keep_list.append(temp)\n",
    "        type_list.append(df['type'][i])\n",
    "    if i%1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keep_list))\n",
    "print(len(type_list))\n",
    "\n",
    "\n",
    "uniform_df = df[df[\"num_posts\"] == 50]\n",
    "uniform_df.reset_index(drop=False)\n",
    "print(uniform_df)\n",
    "\n",
    "frame = pd.DataFrame({\n",
    "    'posts' : keep_list,\n",
    "    'type'  : type_list\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each post to remove links\n",
    "import re\n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "for posts in frame.posts:\n",
    "    for i in range(len(posts)):\n",
    "        posts[i] = pattern.sub('link', posts[i])\n",
    "        \n",
    "print(frame.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7684\n"
     ]
    }
   ],
   "source": [
    "def type_to_numbers(type_list):\n",
    "    \n",
    "    label_list = []\n",
    "    for i in type_list:\n",
    "        if i == 'ENTP':\n",
    "            label_list.append(0)\n",
    "        if i == 'INTP':\n",
    "            label_list.append(7)\n",
    "        if i == 'ESTP':\n",
    "            label_list.append(8)\n",
    "        if i == 'ISTP':\n",
    "            label_list.append(3)\n",
    "        if i == 'ENFP':\n",
    "            label_list.append(4)\n",
    "        if i == 'INFP':\n",
    "            label_list.append(5)\n",
    "        if i == 'ESFP':\n",
    "            label_list.append(6)\n",
    "        if i == 'ISFP':\n",
    "            label_list.append(7)\n",
    "        if i == 'ENTJ':\n",
    "            label_list.append(8)\n",
    "        if i == 'INTJ':\n",
    "            label_list.append(9)\n",
    "        if i == 'ESTJ':\n",
    "            label_list.append(10)\n",
    "        if i == 'ISTJ':\n",
    "            label_list.append(11)\n",
    "        if i == 'ENFJ':\n",
    "            label_list.append(12)\n",
    "        if i == 'INFJ':\n",
    "            label_list.append(13)\n",
    "        if i == 'ESFJ':\n",
    "            label_list.append(14)\n",
    "        if i == 'ISFJ':\n",
    "            label_list.append(15)\n",
    "        \n",
    "    return label_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "frame['label'] = type_to_numbers(type_list)\n",
    "    \n",
    "print(len(type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7684\n"
     ]
    }
   ],
   "source": [
    "def type_to_numbers(type_list):\n",
    "    \n",
    "    hot_label_list = []\n",
    "    for i in type_list:\n",
    "        if i == 'ENTP':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 1, 0, 1, 0]))\n",
    "        if i == 'INTP':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 1, 0, 1, 0]))\n",
    "        if i == 'ESTP':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 1, 0, 1, 0]))\n",
    "        if i == 'ISTP':\n",
    "            hot_label_list.append(np.array([ 0, 1, 0, 1, 1, 0, 1, 0]))\n",
    "        if i == 'ENFP':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 0, 1, 1, 0]))\n",
    "        if i == 'INFP':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 0, 1, 1, 0]))\n",
    "        if i == 'ESFP':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 0, 1, 1, 0]))\n",
    "        if i == 'ISFP':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 0, 1, 1, 0]))\n",
    "        if i == 'ENTJ':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 1, 0, 0, 1]))\n",
    "        if i == 'INTJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ESTJ':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ISTJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 1, 0, 0, 1]))\n",
    "        if i == 'ENFJ':\n",
    "            hot_label_list.append(np.array([1, 0, 1, 0, 0, 1, 0, 1]))\n",
    "        if i == 'INFJ':\n",
    "            hot_label_list.append(np.array([0, 1, 1, 0, 0, 1, 0, 1]))\n",
    "        if i == 'ESFJ':\n",
    "            hot_label_list.append(np.array([1, 0, 0, 1, 0, 1, 0, 1]))\n",
    "        if i == 'ISFJ':\n",
    "            hot_label_list.append(np.array([0, 1, 0, 1, 0, 1, 0, 1]))\n",
    "        \n",
    "    return hot_label_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "frame['hot_label'] = type_to_numbers(type_list)\n",
    "    \n",
    "print(len(type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25%\n",
      "25%\n",
      "25%\n",
      "25%\n"
     ]
    }
   ],
   "source": [
    "posts_vector_2d_list = []\n",
    "posts_vector_1d_list = []\n",
    "\n",
    "for posts in frame.posts:\n",
    "    p = np.empty((50,300))\n",
    "    all_posts = \"\"\n",
    "    for i in range(len(posts)): \n",
    "        string = posts[i].tostring()\n",
    "        s = string.decode('UTF-32')\n",
    "        all_posts += s + \". \"\n",
    "        post_doc = nlp(s)\n",
    "        p[i] = post_doc.vector\n",
    "    all_posts_doc = nlp(all_posts)\n",
    "    posts_vector_2d_list.append(p)\n",
    "    posts_vector_1d_list.append(all_posts_doc.vector)\n",
    "    if len(posts_vector_1d_list)%1921 == 0:\n",
    "        print(\"25%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7684\n",
      "7684\n"
     ]
    }
   ],
   "source": [
    "print(len(posts_vector_2d_list))\n",
    "print(len(posts_vector_1d_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separated into 2 lists:\n",
    "## 1. with a 1,300 word vector representing the average of all the word vectors from all the words in the person's post of length 7684\n",
    "## 2. with a 50,300 word vector representing 50 posts where the average of the word vectors from each posts of a person is 1,300 and also has a length of 7684\n",
    "\n",
    "### See the pd.DataFrames below to see how they are saved\n",
    "### Notice I also have encoded the 16 types into just simple numbers from 1-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              posts\n",
      "0     14  [-0.02311873, 0.13927716, -0.16686235, -0.0479...\n",
      "1      1  [-0.02635534, 0.15930974, -0.20861474, -0.0925...\n",
      "2      2  [-0.033611614, 0.183018, -0.18702613, -0.08562...\n",
      "3     10  [-0.034763753, 0.18936357, -0.19607021, -0.088...\n",
      "4      9  [-0.081055716, 0.15768416, -0.18289337, -0.093...\n"
     ]
    }
   ],
   "source": [
    "word_vectors_df = pd.DataFrame({\n",
    "    'posts' : posts_vector_1d_list,\n",
    "    'label' : label_list\n",
    "})\n",
    "\n",
    "print(word_vectors_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              posts\n",
      "0     14  [[-0.2548750042915344, 0.14718499779701233, -0...\n",
      "1      1  [[-0.07861513644456863, 0.2492658644914627, -0...\n",
      "2      2  [[-0.014407179318368435, 0.03320727497339249, ...\n",
      "3     10  [[0.0036523607559502125, 0.0886211022734642, -...\n",
      "4      9  [[-0.1215578094124794, 0.3519960045814514, -0....\n"
     ]
    }
   ],
   "source": [
    "word_tensors_df = pd.DataFrame({\n",
    "    'posts' : posts_vector_2d_list,\n",
    "    'label' : label_list\n",
    "})\n",
    "\n",
    "print(word_tensors_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.to_csv(\"word_vectors.csv\")\n",
    "word_tensors_df.to_csv(\"word_tenors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  label                                              posts\n",
      "0           0     14  [-2.31187306e-02  1.39277160e-01 -1.66862354e-...\n",
      "1           1      1  [-2.63553392e-02  1.59309745e-01 -2.08614737e-...\n",
      "2           2      2  [-3.36116143e-02  1.83017999e-01 -1.87026128e-...\n",
      "3           3     10  [-3.47637534e-02  1.89363569e-01 -1.96070209e-...\n",
      "4           4      9  [-8.10557157e-02  1.57684162e-01 -1.82893366e-...\n"
     ]
    }
   ],
   "source": [
    "wvdf = pd.read_csv(\"word_vectors.csv\")\n",
    "wtdf = pd.read_csv(\"word_tenors.csv\")\n",
    "print(wvdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data includes looping through each post from each person and separating them into their 50 statements at the |||. Then remove the links and keep count for data visualizations later. Take the MBTI type and break it into 0/1 for I/E, S/N, F/T, and J/P respectively so we can predict individual features and functions of personality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The process for which metrics, algorithms, and techniques were implemented with the given datasets or input data has been thoroughly documented. Complications that occurred during the coding process are discussed.\n",
    "\n",
    "1. Preprocess the data, splitting the post string into an array of posts and removing any people with less than 50 posts, since the keras NN must be uniform.\n",
    "2. Remove the links\n",
    "3. Process each post with SpaCy nlp library turning each post into an average word vector over the words of the post.\n",
    "4. Save a df of all the posts and their respective person's type for simple classifiers\n",
    "5. Save a df of each person and their 50, 300 vector of 50 1, 300 posts for keras\n",
    "5. Split the posts_df into train and test sets\n",
    "6. Train on Logistic Regression, Random Forrest, and MLPClassifier\n",
    "7. Test accuracy and AUC of different categories\n",
    "8. Cross validate\n",
    "9. Split persons_word_vector_df into train, valid, and test datasets\n",
    "10. Input into keras\n",
    "11. Test accuracy \n",
    "12. Cross validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "The process of improving upon the algorithms and techniques used is clearly documented. Both the initial and final solutions are reported, along with intermediate solutions, if necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation\n",
    "\n",
    "The final model’s qualities — such as parameters — are evaluated in detail. Some type of analysis is used to validate the robustness of the model’s solution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "\n",
    "The final results are compared to the benchmark result or threshold with some type of statistical analysis. Justification is made as to whether the final model and solution is significant enough to have adequately solved the problem.\n",
    "\n",
    "Although the final model performed much better than the benchmark, I would not say this solution adequately solves the problem of developing a tool to help understand people's personality through their language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['e'] = 0\n",
    "data['n'] = 0\n",
    "data['t'] = 0\n",
    "data['p'] = 0\n",
    "\n",
    "    \n",
    "for i in range(len(data.type)): #\n",
    "    if data.type[i]:\n",
    "        print(data.type[i])\n",
    "        if data.type[i][0] == 'E':\n",
    "            data['e'][i] = 1\n",
    "        if data.type[i][1] == 'N':\n",
    "            data['n'][i] = 1\n",
    "        if data.type[i][2] == 'T':\n",
    "            data['t'][i] = 1\n",
    "        if data.type[i][3] == 'P':\n",
    "            data['p'][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I am creating my simple models with the 7684 length list of 1,300 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html\n",
    "#scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "for i in range(len(x)):\n",
    "    x_list.append(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes I don't do type. Instead, I'll predict data['t'] and calculate AUC, clearly IN are bias in this data set\n",
    "# type_by_post_list\n",
    "X_train, X_test, y_train, y_test = train_test_split(posts_vector_1d_list, label_list, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "nn_clf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.3482561166059344\n",
      "RandomForestClassifier 0.24622592399791776\n",
      "MLPClassifier 0.478396668401874\n"
     ]
    }
   ],
   "source": [
    "#X_test_train, X_test_t, y_test_train, y_test_t = train_test_split(X_test, y_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "#print(X_train.shape)\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)    \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    #auc = metrics.auc(fpr, tpr)\n",
    "    #print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am getting about 48% for MLP's which is better than random guessing (6.25% chance) or highest frequency class (about 21%)\n",
    "\n",
    "## Notice I have to reshape the vector below to get a prediction, I don't quite understand why and the shape may have to do with the problems below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Whattup')\n",
    "tester = doc.vector.reshape(1, -1)\n",
    "prediction = nn_clf.predict(tester)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I can't seem to get the labels one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 1 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-266-0c2569ff7ffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 1 with size 16"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(num_classes)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# break training set into training and validation sets\n",
    "(x_train, x_valid) = X_train[2000:], X_train[:2000]\n",
    "(y_train, y_valid) = y_train[2000:], y_train[:2000]\n",
    "\n",
    "# print shape of training set\n",
    "#print('X_train shape:', X_train.shape)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "#print(x_train.shape[0], 'train samples')\n",
    "#print(x_test.shape[0], 'test samples')\n",
    "#print(x_valid.shape[0], 'validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I tried adopting this function to my dataset but I can't figure out what's even happening to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1843500 into shape (6145,1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-d7921cfd958f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtestX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    255\u001b[0m            [5, 6]])\n\u001b[0;32m    256\u001b[0m     \"\"\"\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1843500 into shape (6145,1,1)"
     ]
    }
   ],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "train_size = int(len(posts_vector_1d_list) * 0.80)\n",
    "test_size = len(posts_vector_1d_list) - train_size\n",
    "train, test = posts_vector_1d_list[0:train_size], posts_vector_1d_list[train_size:len(posts_vector_1d_list)]\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_101 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 118,924\n",
      "Trainable params: 118,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM\n",
    "model3 = Sequential()\n",
    "#model3.add(Embedding(6147, 300))  # Don't understand this part. What am I trying to input here?\n",
    "#model3.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))\n",
    "#model3.add(Dense(16,activation='relu'))\n",
    "\n",
    "model3.add(Dense(300, activation='relu', input_shape=(300,)))\n",
    "#model3.add(Dense(256, activation='relu'))\n",
    "#model3.add(Dense(128, activation='relu'))\n",
    "#model3.add(Dropout(0.2))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(16, activation='softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(frame['label'], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5763, 300)\n",
      "(5763, 16)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(posts_vector_1d_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y_binary, test_size=0.25, random_state=1, shuffle=True)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4034 samples, validate on 1729 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 1.9299 - acc: 0.3000 - val_loss: 1.9518 - val_acc: 0.2840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.95182, saving model to CNN.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 1.9244 - acc: 0.3039 - val_loss: 1.9552 - val_acc: 0.2828\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.9143 - acc: 0.3166 - val_loss: 1.9487 - val_acc: 0.2909\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.95182 to 1.94872, saving model to CNN.weights.best.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.9229 - acc: 0.3079 - val_loss: 1.9528 - val_acc: 0.2967\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      " - 2s - loss: 1.9040 - acc: 0.3024 - val_loss: 1.8979 - val_acc: 0.3094\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.94872 to 1.89786, saving model to CNN.weights.best.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 1.9095 - acc: 0.3133 - val_loss: 1.9010 - val_acc: 0.3054\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/100\n",
      " - 2s - loss: 1.9002 - acc: 0.3260 - val_loss: 1.9233 - val_acc: 0.3065\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      " - 2s - loss: 1.8983 - acc: 0.3178 - val_loss: 1.8968 - val_acc: 0.3141\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.89786 to 1.89685, saving model to CNN.weights.best.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 1.8911 - acc: 0.3190 - val_loss: 1.8911 - val_acc: 0.3065\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.89685 to 1.89110, saving model to CNN.weights.best.hdf5\n",
      "Epoch 10/100\n",
      " - 2s - loss: 1.8701 - acc: 0.3275 - val_loss: 1.9741 - val_acc: 0.3054\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      " - 2s - loss: 1.8817 - acc: 0.3148 - val_loss: 1.9313 - val_acc: 0.3216\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      " - 2s - loss: 1.8723 - acc: 0.3240 - val_loss: 2.0324 - val_acc: 0.2869\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      " - 2s - loss: 1.8892 - acc: 0.3138 - val_loss: 1.8805 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.89110 to 1.88055, saving model to CNN.weights.best.hdf5\n",
      "Epoch 14/100\n",
      " - 2s - loss: 1.8765 - acc: 0.3272 - val_loss: 1.9021 - val_acc: 0.3343\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      " - 2s - loss: 1.8610 - acc: 0.3359 - val_loss: 1.8815 - val_acc: 0.3216\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      " - 2s - loss: 1.8563 - acc: 0.3309 - val_loss: 1.8754 - val_acc: 0.3279\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.88055 to 1.87542, saving model to CNN.weights.best.hdf5\n",
      "Epoch 17/100\n",
      " - 2s - loss: 1.8522 - acc: 0.3277 - val_loss: 1.9041 - val_acc: 0.3152\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      " - 2s - loss: 1.8481 - acc: 0.3342 - val_loss: 1.8738 - val_acc: 0.3320\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.87542 to 1.87382, saving model to CNN.weights.best.hdf5\n",
      "Epoch 19/100\n",
      " - 2s - loss: 1.8335 - acc: 0.3342 - val_loss: 1.8760 - val_acc: 0.3222\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      " - 2s - loss: 1.8505 - acc: 0.3352 - val_loss: 1.9315 - val_acc: 0.3216\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      " - 2s - loss: 1.8438 - acc: 0.3433 - val_loss: 1.8825 - val_acc: 0.3326\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      " - 2s - loss: 1.8412 - acc: 0.3354 - val_loss: 1.9316 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      " - 2s - loss: 1.8365 - acc: 0.3411 - val_loss: 1.8850 - val_acc: 0.3268\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      " - 2s - loss: 1.8380 - acc: 0.3349 - val_loss: 1.9230 - val_acc: 0.3239\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      " - 2s - loss: 1.8206 - acc: 0.3446 - val_loss: 1.8606 - val_acc: 0.3366\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.87382 to 1.86056, saving model to CNN.weights.best.hdf5\n",
      "Epoch 26/100\n",
      " - 2s - loss: 1.8403 - acc: 0.3471 - val_loss: 1.8593 - val_acc: 0.3383\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.86056 to 1.85928, saving model to CNN.weights.best.hdf5\n",
      "Epoch 27/100\n",
      " - 2s - loss: 1.8381 - acc: 0.3505 - val_loss: 1.8549 - val_acc: 0.3314\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.85928 to 1.85494, saving model to CNN.weights.best.hdf5\n",
      "Epoch 28/100\n",
      " - 2s - loss: 1.8061 - acc: 0.3500 - val_loss: 1.8679 - val_acc: 0.3366\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      " - 2s - loss: 1.8236 - acc: 0.3431 - val_loss: 1.9383 - val_acc: 0.3476\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      " - 2s - loss: 1.8232 - acc: 0.3453 - val_loss: 1.8754 - val_acc: 0.3320\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      " - 2s - loss: 1.8252 - acc: 0.3458 - val_loss: 1.8465 - val_acc: 0.3424\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.85494 to 1.84647, saving model to CNN.weights.best.hdf5\n",
      "Epoch 32/100\n",
      " - 2s - loss: 1.8194 - acc: 0.3438 - val_loss: 1.8951 - val_acc: 0.3302\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      " - 2s - loss: 1.7995 - acc: 0.3545 - val_loss: 1.8370 - val_acc: 0.3401\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.84647 to 1.83699, saving model to CNN.weights.best.hdf5\n",
      "Epoch 34/100\n",
      " - 2s - loss: 1.7870 - acc: 0.3612 - val_loss: 1.9482 - val_acc: 0.3326\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      " - 2s - loss: 1.7867 - acc: 0.3552 - val_loss: 1.8465 - val_acc: 0.3522\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      " - 2s - loss: 1.7860 - acc: 0.3510 - val_loss: 1.8450 - val_acc: 0.3395\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      " - 2s - loss: 1.7879 - acc: 0.3669 - val_loss: 1.8801 - val_acc: 0.3418\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      " - 2s - loss: 1.8078 - acc: 0.3508 - val_loss: 1.8596 - val_acc: 0.3297\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      " - 2s - loss: 1.7770 - acc: 0.3520 - val_loss: 1.8506 - val_acc: 0.3308\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      " - 2s - loss: 1.7996 - acc: 0.3577 - val_loss: 1.9130 - val_acc: 0.3349\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      " - 2s - loss: 1.7943 - acc: 0.3614 - val_loss: 1.8482 - val_acc: 0.3424\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      " - 2s - loss: 1.7818 - acc: 0.3562 - val_loss: 1.9232 - val_acc: 0.3488\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      " - 2s - loss: 1.7733 - acc: 0.3577 - val_loss: 1.8704 - val_acc: 0.3412\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      " - 2s - loss: 1.7814 - acc: 0.3585 - val_loss: 1.8432 - val_acc: 0.3355\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      " - 2s - loss: 1.7506 - acc: 0.3674 - val_loss: 1.8537 - val_acc: 0.3418\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      " - 2s - loss: 1.7627 - acc: 0.3617 - val_loss: 1.8355 - val_acc: 0.3522\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.83699 to 1.83551, saving model to CNN.weights.best.hdf5\n",
      "Epoch 47/100\n",
      " - 2s - loss: 1.7654 - acc: 0.3731 - val_loss: 1.8394 - val_acc: 0.3493\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      " - 2s - loss: 1.7609 - acc: 0.3599 - val_loss: 1.8549 - val_acc: 0.3447\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      " - 2s - loss: 1.7535 - acc: 0.3701 - val_loss: 1.9989 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      " - 2s - loss: 1.7877 - acc: 0.3604 - val_loss: 1.9202 - val_acc: 0.3372\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      " - 2s - loss: 1.7940 - acc: 0.3577 - val_loss: 1.8617 - val_acc: 0.3493\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      " - 2s - loss: 1.7802 - acc: 0.3592 - val_loss: 1.8817 - val_acc: 0.3482\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      " - 2s - loss: 1.7590 - acc: 0.3765 - val_loss: 1.8544 - val_acc: 0.3470\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      " - 2s - loss: 1.7697 - acc: 0.3669 - val_loss: 1.8241 - val_acc: 0.3569\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.83551 to 1.82409, saving model to CNN.weights.best.hdf5\n",
      "Epoch 55/100\n",
      " - 2s - loss: 1.7643 - acc: 0.3706 - val_loss: 1.8811 - val_acc: 0.3464\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      " - 2s - loss: 1.7442 - acc: 0.3751 - val_loss: 1.8498 - val_acc: 0.3464\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      " - 2s - loss: 1.7431 - acc: 0.3758 - val_loss: 1.8404 - val_acc: 0.3493\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      " - 2s - loss: 1.7516 - acc: 0.3768 - val_loss: 1.8627 - val_acc: 0.3262\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      " - 2s - loss: 1.7363 - acc: 0.3798 - val_loss: 1.8186 - val_acc: 0.3534\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00059: val_loss improved from 1.82409 to 1.81855, saving model to CNN.weights.best.hdf5\n",
      "Epoch 60/100\n",
      " - 2s - loss: 1.7304 - acc: 0.3783 - val_loss: 1.8310 - val_acc: 0.3522\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      " - 2s - loss: 1.7226 - acc: 0.3862 - val_loss: 1.8219 - val_acc: 0.3574\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      " - 2s - loss: 1.7271 - acc: 0.3830 - val_loss: 1.8312 - val_acc: 0.3511\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      " - 2s - loss: 1.7315 - acc: 0.3837 - val_loss: 1.8112 - val_acc: 0.3534\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.81855 to 1.81121, saving model to CNN.weights.best.hdf5\n",
      "Epoch 64/100\n",
      " - 2s - loss: 1.7146 - acc: 0.3919 - val_loss: 1.8474 - val_acc: 0.3580\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      " - 2s - loss: 1.7306 - acc: 0.3785 - val_loss: 1.8339 - val_acc: 0.3592\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      " - 2s - loss: 1.7170 - acc: 0.3832 - val_loss: 1.9356 - val_acc: 0.3216\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      " - 2s - loss: 1.7294 - acc: 0.3830 - val_loss: 1.8240 - val_acc: 0.3499\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      " - 2s - loss: 1.7182 - acc: 0.3974 - val_loss: 1.8316 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      " - 2s - loss: 1.7244 - acc: 0.3847 - val_loss: 1.8095 - val_acc: 0.3563\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.81121 to 1.80949, saving model to CNN.weights.best.hdf5\n",
      "Epoch 70/100\n",
      " - 2s - loss: 1.7319 - acc: 0.3850 - val_loss: 1.8122 - val_acc: 0.3505\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      " - 2s - loss: 1.7153 - acc: 0.3842 - val_loss: 1.9943 - val_acc: 0.3146\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      " - 2s - loss: 1.7362 - acc: 0.3872 - val_loss: 1.8726 - val_acc: 0.3360\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      " - 2s - loss: 1.7133 - acc: 0.3887 - val_loss: 1.9977 - val_acc: 0.3193\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      " - 2s - loss: 1.7126 - acc: 0.3909 - val_loss: 1.8429 - val_acc: 0.3638\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      " - 2s - loss: 1.7115 - acc: 0.3872 - val_loss: 1.8305 - val_acc: 0.3626\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      " - 2s - loss: 1.7050 - acc: 0.3971 - val_loss: 1.8196 - val_acc: 0.3655\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      " - 2s - loss: 1.6911 - acc: 0.3964 - val_loss: 1.8206 - val_acc: 0.3673\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      " - 2s - loss: 1.6995 - acc: 0.3954 - val_loss: 1.8665 - val_acc: 0.3661\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      " - 2s - loss: 1.7313 - acc: 0.3823 - val_loss: 1.8693 - val_acc: 0.3609\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      " - 2s - loss: 1.6845 - acc: 0.3956 - val_loss: 1.8197 - val_acc: 0.3730\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      " - 2s - loss: 1.6979 - acc: 0.3954 - val_loss: 1.8426 - val_acc: 0.3551\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      " - 2s - loss: 1.6840 - acc: 0.4013 - val_loss: 1.9125 - val_acc: 0.3609\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      " - 2s - loss: 1.6977 - acc: 0.3971 - val_loss: 1.8246 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      " - 2s - loss: 1.6954 - acc: 0.4001 - val_loss: 1.8458 - val_acc: 0.3516\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      " - 2s - loss: 1.6809 - acc: 0.3939 - val_loss: 1.8759 - val_acc: 0.3511\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      " - 2s - loss: 1.6920 - acc: 0.4065 - val_loss: 1.8012 - val_acc: 0.3788\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.80949 to 1.80123, saving model to CNN.weights.best.hdf5\n",
      "Epoch 87/100\n",
      " - 2s - loss: 1.6851 - acc: 0.4016 - val_loss: 1.7957 - val_acc: 0.3783\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.80123 to 1.79567, saving model to CNN.weights.best.hdf5\n",
      "Epoch 88/100\n",
      " - 2s - loss: 1.6694 - acc: 0.4008 - val_loss: 1.9048 - val_acc: 0.3256\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      " - 2s - loss: 1.7485 - acc: 0.3795 - val_loss: 1.8457 - val_acc: 0.3713\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      " - 2s - loss: 1.6887 - acc: 0.3897 - val_loss: 1.9063 - val_acc: 0.3360\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      " - 2s - loss: 1.6927 - acc: 0.4018 - val_loss: 1.8220 - val_acc: 0.3817\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/100\n",
      " - 2s - loss: 1.6841 - acc: 0.4026 - val_loss: 1.8848 - val_acc: 0.3592\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      " - 2s - loss: 1.6816 - acc: 0.4048 - val_loss: 1.7963 - val_acc: 0.3806\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      " - 2s - loss: 1.6528 - acc: 0.4167 - val_loss: 1.7811 - val_acc: 0.3829\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.79567 to 1.78107, saving model to CNN.weights.best.hdf5\n",
      "Epoch 95/100\n",
      " - 2s - loss: 1.6854 - acc: 0.4048 - val_loss: 1.8002 - val_acc: 0.3783\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/100\n",
      " - 2s - loss: 1.6821 - acc: 0.4051 - val_loss: 1.8129 - val_acc: 0.3748\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      " - 2s - loss: 1.6697 - acc: 0.4083 - val_loss: 1.8082 - val_acc: 0.3783\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      " - 2s - loss: 1.6718 - acc: 0.4142 - val_loss: 1.8023 - val_acc: 0.3858\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      " - 2s - loss: 1.6846 - acc: 0.4080 - val_loss: 1.8069 - val_acc: 0.3771\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      " - 2s - loss: 1.6997 - acc: 0.3946 - val_loss: 1.8203 - val_acc: 0.3852\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#train the model\n",
    "checkpointer = ModelCheckpoint(filepath='CNN.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model3.fit(X_train, y_train, batch_size = 32, epochs = 100, validation_data=(X_val, y_val), callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weights that had the best validation score\n",
    "\n",
    "model3.load_weights('CNN.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy:  0.5023425299323269\n",
      "[1.595282998201687, 0.5023425299323269]\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(X_test, y_test, verbose = 0)\n",
    "print('\\nTest accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Anything below is just code from me messing around too much and getting off task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'e'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-13461c1e30a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEN_word_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mextroverts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mintroverts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'e'"
     ]
    }
   ],
   "source": [
    "EN_word_vector = []\n",
    "\n",
    "extroverts = data.loc[data['e'] == 1]\n",
    "introverts = data.loc[data['e'] == 0]\n",
    "\n",
    "intuitive_introverts = introverts.loc[data['n'] == 1]\n",
    "intuitive_extroverts = extroverts.loc[data['n'] == 1]\n",
    "\n",
    "for i in intuitive_extroverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in intuitive_extroverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    EN_word_vector.append(doc.vector)\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(EN_word_vector, intuitive_extroverts['t'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "nn_clf = MLPClassifier()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "    \n",
    "    \n",
    "IN_word_vector = []\n",
    "\n",
    "for i in intuitive_introverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in intuitive_introverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    IN_word_vector.append(doc.vector)\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(IN_word_vector, intuitive_introverts['t'], test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "nn_clf = MLPClassifier()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    y_pred = clf.predict(E_word_vector)\n",
    "    print(clf.__class__.__name__, accuracy_score(extroverts['t'], y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(extroverts['t'], y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "    \n",
    "    \n",
    "E_word_vector = []\n",
    "for i in extroverts.index:\n",
    "    persons_comments = \"\"\n",
    "    for post in extroverts.posts[i]:\n",
    "        persons_comments += post + \". \"\n",
    "    doc = nlp(persons_comments)\n",
    "    E_word_vector.append(doc.vector)\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "        \n",
    "y_pred = nn_clf.predict(E_word_vector)\n",
    "print(clf.__class__.__name__, accuracy_score(extroverts['p'], y_pred))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(extroverts['p'], y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusion\n",
    "\n",
    "### Free Form Visualization\n",
    "\n",
    "A visualization has been provided that emphasizes an important quality about the project with thorough discussion. Visual cues are clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Student adequately summarizes the end-to-end problem solution and discusses one or two particular aspects of the project they found interesting or difficult.\n",
    "\n",
    "One particular thing I found difficult was preprocessing the data for the Keras/ Tensorflow Neural Network.  For the simple classifiers I had to input a 1D stream of data, whereas I needed 3 dimensions for the LSTM NN. At first I was getting very confused, going back and forth between different preprocessing methods getting one to fit and then not the other. I did not realize that I needed to make two separate datasets in order to make this project work.  However, what I found interesting was that the SpaCy word vectors take into account dependencies and time series when creating word vectors. Since this is information is encapsulated, then the LSTM can focus on more broad patterns than one understanding and predicting the likelihood of seeing the phrase.  Finding the right mix of abstraction of language without losing information in the averaged word vectors will be integral in understanding the patterns of the human psyche through language.\n",
    "\n",
    "Another interesting thing I had trouble with was one-hot encoding the personality data. Since there are 16 different personality types, one method would be to make each prediction class equivalent to one personality type resulting in an array of length 16. Another way would be to predict whether or not each person was Introverted, Extroverted, iNtuitive, Sensing, Thinking, Feeling, Perceiving, or Judging, resulting in an array of length 8.  Or even take it one step further, since one cannot be both Introverted and Extroverted according to the MBTI, we could assign Extroverted a value of 1 and Introverted a value of 0.  This would result in an array of length 4. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "\t\n",
    "Discussion is made as to how one aspect of the implementation could be improved. Potential solutions resulting from these improvements are considered and compared/contrasted to the current solution.\n",
    "\n",
    "One aspect of the implementation that could have been improved is cleaning the data in a way that didn't require losing people with less than 50 posts and any post after the 50th one, losing information from links, or result in a skewed distribution of classes in the dataset. \n",
    "\n",
    "This way, the most information would be preserved by the corpus of the users and the model would result in a more transferrable prediction to the general population, rather than specifically Kaggle members.\n",
    "\n",
    "I could also get the tensor for each post or corpus the person has, this would increase the dimensionality of the data further and allow for a convolutional neural network, however, this takes more than 2 days to process on my quad-core processor and requires padding or otherwise losing information in some posts as some people use more words than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_train, X_test_t, y_test_train, y_test_t = train_test_split(X_test, y_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, nn_clf): #svm_clf,\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test_train)    \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test_train, y_pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_train, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)\n",
    "\n",
    "nn_clf_2 = MLPClassifier()\n",
    "#train_np = np.array(X_test_train)\n",
    "#print(len(train_np))\n",
    "#guess = y_pred.tolist()\n",
    "\n",
    "print(X_test_train[0])\n",
    "\n",
    "array = y_pred.reshape(-1, 1)\n",
    "df = pd.DataFrame ({\n",
    "    'vector': np.array(X_test_train,dtype='float32'),\n",
    "    'pred'  : array\n",
    "})\n",
    "\n",
    "#print(X_test_train[:2])\n",
    "#trainer = zip(X_test_train, guess)\n",
    "#xtestrain = np.ndarray(list(trainer))\n",
    "#print(xtestrain)\n",
    "nn_clf_2.fit(array, y_test_train)\n",
    "y_pred_2 = nn_clf_2.predict(df)\n",
    "print(clf.__class__.__name__, accuracy_score(y_test_t, y_pred_2))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_train, y_pred_2, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_list = []\n",
    "df_label_list = []\n",
    "df_vector_norm_list = []\n",
    "df_post_list = []\n",
    "df_index_list = []\n",
    "e_list = []\n",
    "n_list = []\n",
    "t_list = [] \n",
    "p_list = []\n",
    "word_vectors = []\n",
    "vector_norms = []\n",
    "\n",
    "\n",
    "for i in range(len(data.type)):\n",
    "    # go through each person in the dataframe, with a fresh list of vectors for \n",
    "    #persons_comments = \"\"\n",
    "    #list_of_vectors = []\n",
    "    #list_of_vector_norms = []\n",
    "    persons_comments = \"\"\n",
    "    for post in data.posts[i]:\n",
    "        \n",
    "    label = [data.iloc[i]['e'], data.iloc[i]['n'], data.iloc[i]['t'], data.iloc[i]['p']]\n",
    "\n",
    "    for post in data.posts[i]:\n",
    "        #persons_comments += post + \". \"\n",
    "        persons_comments += post + \" \"\n",
    "\n",
    "        doc = nlp(post)\n",
    "        df_vector_list.append(doc.vector)\n",
    "        df_vector_norm_list.append(doc.vector_norm)\n",
    "        df_post_list.append(post)\n",
    "        df_index_list.append(i)\n",
    "        df_label_list.append(label)\n",
    "        e_list.append(label[0])\n",
    "        n_list.append(label[1])\n",
    "        t_list.append(label[2]) \n",
    "        p_list.append(label[3])\n",
    "    # after each post has been gone through but before the moving on to the next person\n",
    "        # I want a list of vector norms to append to df vector norm list\n",
    "        # I want a vector describing the whole corpus\n",
    "    #vector.append(doc.vector)\n",
    "    #print(vectors.dtype)\n",
    "    #data['word_vectors'][i] = vectors\n",
    "    doc = nlp(persons_comments)\n",
    "    word_vectors.append(doc.vector)\n",
    "    vector_norms.append(doc.vector_norm)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675.)if i%500 == 0:\n",
    "        print(i)\n",
    "\n",
    "post_data = { 'user_id' :  df_index_list,\n",
    "              'label'   :  df_label_list,\n",
    "        'word_vector'   :  df_vector_list,\n",
    "        'vector_norm'   :  df_vector_norm_list,\n",
    "             'post'     :  df_post_list,\n",
    "             'links'    :  df_links_list\n",
    "             'e'        :  e_list,\n",
    "             'n'        :  n_list,\n",
    "             't'        :  t_list,\n",
    "             'p'        :  p_list\n",
    "            }\n",
    "post_df = pd.DataFrame.from_dict(post_data)\n",
    "print(post_df.head())\n",
    "\n",
    "for person in data.type:\n",
    "    if check_E(person):\n",
    "        if check_N(person):\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(1) #[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ENTP\n",
    "                else:\n",
    "                    label_list.append(2) #[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(3) #[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENFP\n",
    "                else:\n",
    "                    label_list.append(4) #[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ENFJ\n",
    "                \n",
    "        else:\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(5) #[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESTP\n",
    "                else:\n",
    "                    label_list.append(6) #[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(7) #[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] # ESFP\n",
    "                else:\n",
    "                    label_list.append(8) #[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] # ESFJ\n",
    "    else:\n",
    "        if check_N(person):\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(9) #[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # INTP\n",
    "                else:\n",
    "                    label_list.append(10) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] # INTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(11) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] # INFP\n",
    "                else:\n",
    "                    label_list.append(12) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] # INFJ\n",
    "                \n",
    "        else:\n",
    "            if check_T:\n",
    "                if check_P:\n",
    "                    label_list.append(13) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] # ISTP\n",
    "                else:\n",
    "                    label_list.append(14) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] # ISTJ\n",
    "            else:\n",
    "                if check_P:\n",
    "                    label_list.append(15) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] # ISFP\n",
    "                else:\n",
    "                    label_list.append(16) #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] # ISFJ\n",
    "    \n",
    "print(label_list)\n",
    "\n",
    "\n",
    "high_dim_vectors = []\n",
    "\n",
    "for i in range(10):\n",
    "    persons_hd_vectors = []\n",
    "    persons_comments = \"\"\n",
    "    #    print(data.posts[i])\n",
    "    #for post in data.posts[i]:\n",
    "    #    print(post)\n",
    "    #    persons_comments += post + \" \"\n",
    "    print(data.posts[i][:100])\n",
    "    doc = nlp(data.posts[i][:100])\n",
    "    persons_hd_vectors.append(doc.tensor)\n",
    "    print(persons_hd_vectors[].shape)\n",
    "    \n",
    "# Go through each post, convert it to a doc.vector and it's associated type\n",
    "post_list = []\n",
    "type_by_post_list = []\n",
    "vector = np.empty((7684, 50, 300))\n",
    "vector_list = []\n",
    "i = 0\n",
    "for posts in frame.posts:\n",
    "    t = type_dict[i]\n",
    "    print(t)\n",
    "    p = np.empty((50,300))\n",
    "    for j in range(50):\n",
    "        string = posts[j].tostring()\n",
    "        strn = string.decode('UTF-16')\n",
    "        print(strn)\n",
    "        doc = nlp(strn)\n",
    "        post_list.append(doc.vector)\n",
    "        type_by_post_list.append(t)\n",
    "        p[j] = doc.vector\n",
    "    vector[i] = p\n",
    "    vector_list.append(p)\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "    print(i)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    \n",
    "print(len(post_list))\n",
    "print(len(type_by_post_list)) \n",
    "#print(len(vector))\n",
    "print(len(vector_list))\n",
    "#print(len(type_dict))\n",
    "\n",
    "\n",
    "\n",
    "posts_dict = {\n",
    "    \"label\" : type_by_post_list\n",
    "    \"vector\" : post_list\n",
    "}\n",
    "\n",
    "posts_df = pd.DataFrame.fromdict(posts_dict)\n",
    "print(posts_df.head())\n",
    "\n",
    "import re\n",
    "\n",
    "# Split the data at |||\n",
    "vector = np.empty((8675, 30, 300))\n",
    "simple_vector = np.empty(())\n",
    "drop_list = []\n",
    "\n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "for i in range(len(data.posts)):  \n",
    "    p = np.empty((30,300))\n",
    "    posts = data.posts[i]\n",
    "    #print(posts)\n",
    "    #data.posts[i] = posts.split('|||')\n",
    "    posts = pattern.sub('link', posts)\n",
    "    newnew = posts.split('|||')\n",
    "    if len(newnew) >= 30:\n",
    "        for j in range(30):\n",
    "            doc = nlp(newnew[j])\n",
    "            p[j] = doc.vector\n",
    "        vector[i] = p\n",
    "    else:\n",
    "        #print(i, len(newnew))\n",
    "        drop_list.append(i)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675., \"% done\")\n",
    "        \n",
    "print(data.head())\n",
    "\n",
    "word_vectors = []\n",
    "vector_norms = []\n",
    "data['e'] = 0\n",
    "data['n'] = 0\n",
    "data['t'] = 0\n",
    "data['p'] = 0\n",
    "    \n",
    "for i in range(1): #len(data.type)\n",
    "    persons_comments = \"\"\n",
    "    if data.type[i][0] == 'E':\n",
    "        data['e'][i] = 1\n",
    "    if data.type[i][1] == 'N':\n",
    "        data['n'][i] = 1\n",
    "    if data.type[i][2] == 'T':\n",
    "        data['t'][i] = 1\n",
    "    if data.type[i][3] == 'P':\n",
    "        data['p'][i] = 1\n",
    "    #for post in data.posts[i]:\n",
    "    #    persons_comments += post + \" \"\n",
    "    #doc = nlp(persons_comments)\n",
    "    doc = nlp(data.posts[i])\n",
    "    word_vectors.append(doc.vector)\n",
    "    vector_norms.append(doc.vector_norm)\n",
    "    tester = doc.vector.reshape(1, -1)\n",
    "    print(tester.shape)\n",
    "    if i%1000 == 0:\n",
    "        print(float(i)/8675.)\n",
    "\n",
    "print(data.tail())\n",
    "\n",
    "\n",
    "average_vector = []\n",
    "i=0\n",
    "\n",
    "for posts in keep_list:\n",
    "    post = \"\"\n",
    "    for p in posts:\n",
    "        post += p + \" \"\n",
    "    doc = nlp(post)\n",
    "    average_vector.append(doc.vector)\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2, activation ='relu', input_shape=(50, 1, 300)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(16, activation='softmax')) #softmax #tanh\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
